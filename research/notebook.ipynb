{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import re\n",
    "os.chdir('../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from IPython.display import HTML as html_print\n",
    "from research.utils import load_vocabulary, Tokenizer, Aligner, RelaxedTargetField, OneHotEncoder, bleu\n",
    "from research.scorer import Scorer\n",
    "from research.greedy_optimizer import GreedyOptimizer\n",
    "from research.beam_optimizer import BeamOptimizer\n",
    "from research.continuous_optimizer import ContinuousOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocabulary()\n",
    "tokenizer = Tokenizer()\n",
    "scorer = Scorer()\n",
    "one_hot_encoder = OneHotEncoder(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation scoring\n",
      "English sentence: I think, that machine translation is a very interesting subject.\n",
      "Translation from google translate to score: Ich denke, dass maschinelle Übersetzung ein sehr interessantes Thema ist.\n",
      "Log likehood of translation: -6.253188\n"
     ]
    }
   ],
   "source": [
    "english_sentence = 'I think, that machine translation is a very interesting subject.'\n",
    "german_translation_from_google = 'Ich denke, dass maschinelle Übersetzung ein sehr interessantes Thema ist.'\n",
    "score = scorer.score_texts(english_sentence, german_translation_from_google)\n",
    "print('Translation scoring')\n",
    "print('English sentence:', english_sentence)\n",
    "print('Translation from google translate to score:', german_translation_from_google)\n",
    "print('Log likehood of translation:', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting of next word in translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence: I think, that machine translation is a very interesting subject.\n",
      "Unfinished translation: Ich denke, dass maschinelle Übersetzung ein sehr\n",
      "Top 3 next tokens:  ▁interessante ▁interessant ▁Interessant\n"
     ]
    }
   ],
   "source": [
    "unfinished_translation = 'Ich denke, dass maschinelle Übersetzung ein sehr'\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "german_tokens = tokenizer.tokenize(unfinished_translation)\n",
    "next_word_probs = scorer.next_word_probabilities([english_tokens], [german_tokens])\n",
    "val, ind = next_word_probs.topk(3)\n",
    "print('English sentence:', english_sentence)\n",
    "print('Unfinished translation:', unfinished_translation )\n",
    "print('Top 3 next tokens: ', vocab.itos[ind[0,0].view(-1)], vocab.itos[ind[0,1].view(-1)], vocab.itos[ind[0,2].view(-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: I think, that machine translation is a very interesting subject.\n",
      "Translated tokens ▁Ich ▁denke , ▁dass ▁die ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema ▁ist .\n",
      "Log likehood of translation: -5.41\n"
     ]
    }
   ],
   "source": [
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = GreedyOptimizer(english_sentence)\n",
    "print('Optimizer initialized...')\n",
    "german_tokens = optimizer.optimize()[:-1]\n",
    "score = scorer.score_tokenized_texts([english_tokens], [german_tokens])[0]\n",
    "print('English sentence:', english_sentence)\n",
    "print('Translated tokens', ' '.join(german_tokens))\n",
    "print(f'Log likehood of translation: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: I think, that machine translation is a very interesting subject.\n",
      "Top 15 translations:\n",
      "1. ▁Ich ▁denke , ▁dass ▁die ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -5.41]\n",
      "2. ▁Ich ▁glaube , ▁dass ▁die ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -5.74]\n",
      "3. ▁Ich ▁halte ▁die ▁ maschine lle ▁Übersetzung ▁für ▁ein ▁sehr ▁interessante s ▁Thema . [p = -5.77]\n",
      "4. ▁Ich ▁denke , ▁dass ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -6.25]\n",
      "5. ▁Ich ▁glaube , ▁dass ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -6.65]\n",
      "6. ▁Ich ▁finde , ▁dass ▁die ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -6.70]\n",
      "7. ▁Ich ▁denke , ▁die ▁ maschine lle ▁Übersetzung ▁ist ▁ein ▁sehr ▁interessante s ▁Thema . [p = -6.87]\n",
      "8. ▁Meine s ▁Era chtens ▁ist ▁die ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema . [p = -7.14]\n",
      "9. ▁Ich ▁denke , ▁dass ▁die ▁Maschinen über setzung ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -7.23]\n",
      "10. ▁Ich ▁denke , ▁dass ▁die ▁Übersetzung ▁von ▁Maschinen ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -7.51]\n",
      "11. ▁Ich ▁glaube , ▁dass ▁die ▁Übersetzung ▁von ▁Maschinen ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -7.87]\n",
      "12. ▁Ich ▁denke , ▁dass ▁die ▁Übersetzung ▁der ▁Maschine ▁ein ▁sehr ▁interessante s ▁Thema ▁ist . [p = -8.20]\n",
      "13. ▁Ich ▁denke , ▁dass ▁das ▁Thema ▁der ▁ maschine llen ▁Übersetzung ▁sehr ▁interessant ▁ist . [p = -8.35]\n",
      "14. ▁Ich ▁glaube , ▁dass ▁das ▁Thema ▁der ▁ maschine llen ▁Übersetzung ▁sehr ▁interessant ▁ist . [p = -8.43]\n",
      "15. ▁Ich ▁denke , ▁dass ▁die ▁ maschine lle ▁Übersetzung ▁ein ▁sehr ▁interessante s ▁Thema ▁darstellt . [p = -9.08]\n",
      "Sanity check: log likehood of the best translation = -5.41\n"
     ]
    }
   ],
   "source": [
    "n_beams = 15\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimizer initialized...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tokens], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: log likehood of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of conditional probabilities for each word in different translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropies(i):\n",
    "    plt.title('Entropy for each word in different translations')\n",
    "    plt.xlabel('Word number')\n",
    "    plt.ylabel('Entropy')\n",
    "    trans = translations[i][:-1]\n",
    "    logprobs = scorer.score_probabilities_for_each_word(english_tokens, trans)\n",
    "    probs = np.exp(logprobs)\n",
    "    words = list(range(probs.shape[0]))\n",
    "    entropies = [entropy(probs[k, :]) for k in words]\n",
    "    plt.scatter(words, entropies, label=f'Translation {i}')\n",
    "    print(f'Translation {i}:\\n{\" | \".join(trans)}')\n",
    "    print()\n",
    "    plt.legend()\n",
    "\n",
    "def plot_aligned_entropies(i, j):\n",
    "    plt.title('Entropy for each word in different translations (aligned)')\n",
    "    plt.xlabel('Word number')\n",
    "    plt.ylabel('Entropy')\n",
    "    \n",
    "    aligner = Aligner()\n",
    "    trans_i = translations[i][:-1]\n",
    "    trans_j = translations[j][:-1]\n",
    "    aligned_i, aligned_j = aligner.align(trans_i, trans_j)\n",
    "    \n",
    "    logprobs_i = scorer.score_probabilities_for_each_word(english_tokens, trans_i)\n",
    "    probs_i = np.exp(logprobs_i)\n",
    "    logprobs_j = scorer.score_probabilities_for_each_word(english_tokens, trans_j)\n",
    "    probs_j = np.exp(logprobs_j)\n",
    "\n",
    "    words = np.arange(len(aligned_i))\n",
    "    entropies_i = []\n",
    "    k = 0\n",
    "    for word in aligned_i:\n",
    "        if word == '[PLACEHOLDER]':\n",
    "            entropies_i += [-1.]\n",
    "        else:\n",
    "            entropies_i += [entropy(probs_i[k, :])]\n",
    "            k += 1\n",
    "    entropies_i = np.array(entropies_i)\n",
    "    \n",
    "    entropies_j = []\n",
    "    k = 0\n",
    "    for word in aligned_j:\n",
    "        if word == '[PLACEHOLDER]':\n",
    "            entropies_j += [-1.]\n",
    "        else:\n",
    "            entropies_j += [entropy(probs_j[k, :])]\n",
    "            k += 1\n",
    "    entropies_j = np.array(entropies_j)\n",
    "    \n",
    "    plt.scatter(words[entropies_i >= 0.], entropies_i[entropies_i >= 0.], label=f'Translation {i}')\n",
    "    plt.scatter(words[entropies_j >= 0.], entropies_j[entropies_j >= 0.], label=f'Translation {j}')\n",
    "    print(f'Translation {i}:\\n{\" | \".join(aligned_i)}')\n",
    "    print(f'Translation {j}:\\n{\" | \".join(aligned_j)}')\n",
    "    print()\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_maxs(i):\n",
    "    plt.title('Max for each word in different translations')\n",
    "    plt.xlabel('Word number')\n",
    "    plt.ylabel('Max')\n",
    "    trans = translations[i][:-1]\n",
    "    logprobs = scorer.score_probabilities_for_each_word(english_tokens, trans)\n",
    "    probs = np.exp(logprobs)\n",
    "    words = list(range(probs.shape[0]))\n",
    "    entropies = [np.max(probs[k, :]) for k in words]\n",
    "    plt.scatter(words, entropies, label=f'Translation {i}')\n",
    "    print(f'Translation {i}:\\n{\" | \".join(trans)}')\n",
    "    print()\n",
    "    plt.legend()\n",
    "    \n",
    "def other_candidates(trans_idx, pos, N = 4):\n",
    "    trans = translations[trans_idx][:-1]\n",
    "    logprobs = scorer.score_probabilities_for_each_word(english_tokens, trans)\n",
    "    probs = np.exp(logprobs)\n",
    "    other_good_choices = np.argsort(probs[pos])[:-6:-1]\n",
    "    print('Best candidates on position', pos, ': ', ' '.join([vocab.itos[idx] for idx in other_good_choices]))\n",
    "    print('Coresponding probabilities:', [probs[pos][idx] for idx in other_good_choices])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the best and the worst translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 0:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁die | ▁ | maschine | lle | ▁Übersetzung | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "\n",
      "Translation 9:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁die | ▁Übersetzung | ▁von | ▁Maschinen | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0, 9]:\n",
    "    plot_entropies(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 0:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁die | ▁ | maschine | lle | ▁Übersetzung | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "\n",
      "Translation 9:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁die | ▁Übersetzung | ▁von | ▁Maschinen | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0, 9]:\n",
    "    plot_maxs(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropia z jakiegoś powodu jest bardzo słabą miarą (chyba, że bug). Entropia dla pozycji 0 w zdaniu 9 niewiele różni się od entropi na pozycji 1 w zdaniu 9, a te rozkłady są zupełnie różne (na pozycji 0 mamy tylko 1 prawdopodobną wartość, a na pozycji 1 aż 4). Przykład poniżej:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropies of alligned sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best vs worst translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 0:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁die | ▁ | maschine | lle | ▁Übersetzung | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "Translation 9:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁die | [PLACEHOLDER] | ▁Übersetzung | ▁von | ▁Maschinen | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_aligned_entropies(0, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam-10 vs Greedy translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 0:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁die | ▁ | maschine | lle | ▁Übersetzung | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "Translation 3:\n",
      "▁Ich | ▁denke | , | ▁dass | [PLACEHOLDER] | ▁ | maschine | lle | ▁Übersetzung | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_aligned_entropies(0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 3:\n",
      "▁Ich | ▁denke | , | ▁dass | ▁ | maschine | lle | ▁Übersetzung | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "Translation 4:\n",
      "▁Ich | ▁glaube | , | ▁dass | ▁ | maschine | lle | ▁Übersetzung | ▁ein | ▁sehr | ▁interessante | s | ▁Thema | ▁ist | .\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_aligned_entropies(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 0 :  ▁Ich ▁Das ▁Die ▁Meiner ▁Meine\n",
      "Coresponding probabilities: [0.5988918, 0.060954917, 0.036728892, 0.024799734, 0.024179423]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(9, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 1 :  ▁denke ▁glaube ▁halte ▁finde ▁bin\n",
      "Coresponding probabilities: [0.29839528, 0.25234625, 0.13621552, 0.10573821, 0.043860655]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(9, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróćmy też uwagę na niejednoznaczność tokenów: w miejscu, w którym wybieramy token _ prawdopodobieństwo jest małe, oraz entropia duża. Wynika to najprawdopodobniej z tego, że zamiast wybrać token _ , możemy wybrać cały token _ machinelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other probable candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 1 :  ▁denke ▁glaube ▁halte ▁finde ▁bin\n",
      "Coresponding probabilities: [0.29839528, 0.25234625, 0.13621552, 0.10573821, 0.043860655]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 4 :  ▁die ▁ ▁das ▁Maschinen ▁eine\n",
      "Coresponding probabilities: [0.4021218, 0.112477794, 0.079811126, 0.05037697, 0.028617628]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 5 :  ▁ ▁Übersetzung ▁Maschinen ▁technische ▁Maschinenbau\n",
      "Coresponding probabilities: [0.4092544, 0.16365328, 0.06526854, 0.013711214, 0.0103740515]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 12 :  . ▁Thema ▁Themen ▁Gegenstand ▁Subjekt\n",
      "Coresponding probabilities: [0.45098555, 0.357347, 0.021454206, 0.005650598, 0.0052555017]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(3, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 5 :  lle llen ll ller elle\n",
      "Coresponding probabilities: [0.89663786, 0.026253838, 0.022556117, 0.012132816, 0.004152786]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidates on position 0 :  ▁Ich ▁Das ▁Die ▁Meiner ▁Meine\n",
      "Coresponding probabilities: [0.5988918, 0.060954917, 0.036728892, 0.024799734, 0.024179423]\n"
     ]
    }
   ],
   "source": [
    "other_candidates(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More difficult sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: Children begin by loving their parents; as they grow older they judge them; sometimes they forgive them.\n",
      "Top 30 translations:\n",
      "1. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -11.66]\n",
      "2. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁ihnen . [p = -11.69]\n",
      "3. ▁Kinder ▁lieben ▁ihre ▁Eltern ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁sie . [p = -11.72]\n",
      "4. ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁ihnen . [p = -11.73]\n",
      "5. ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -11.83]\n",
      "6. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁ihnen . [p = -11.85]\n",
      "7. ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁ihnen . [p = -11.89]\n",
      "8. ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁sie . [p = -12.14]\n",
      "9. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁sie . [p = -12.16]\n",
      "10. ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁sie . [p = -12.35]\n",
      "11. ▁Kinder ▁beginnen ▁mit ▁der ▁Liebe ▁ihrer ▁Eltern ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁ihnen . [p = -12.37]\n",
      "12. ▁Die ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -12.38]\n",
      "13. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁sie . [p = -12.45]\n",
      "14. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁da ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -12.49]\n",
      "15. ▁Kinder ▁beginnen ▁mit ▁der ▁Liebe ▁ihrer ▁Eltern ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁ihnen . [p = -12.54]\n",
      "16. ▁Die ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁ihnen . [p = -12.55]\n",
      "17. ▁Kinder ▁beginnen ▁mit ▁der ▁Liebe ▁ihrer ▁Eltern ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -12.55]\n",
      "18. ▁Die ▁Kinder ▁beginnen , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁ihnen . [p = -12.62]\n",
      "19. ▁Die ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -12.63]\n",
      "20. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁da ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁ihnen . [p = -12.64]\n",
      "21. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie , ▁ manchmal ▁vergeben ▁sie ▁ihnen . [p = -12.65]\n",
      "22. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁da ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁ihnen . [p = -12.72]\n",
      "23. ▁Kinder ▁beginnen ▁mit ▁der ▁Liebe ▁ihrer ▁Eltern ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁sie . [p = -12.76]\n",
      "24. ▁Die ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁ihnen . [p = -12.81]\n",
      "25. ▁Die ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁ihnen . [p = -12.85]\n",
      "26. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie . [p = -12.85]\n",
      "27. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁ihnen . [p = -13.04]\n",
      "28. ▁Die ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁sie . [p = -13.10]\n",
      "29. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie , ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -13.17]\n",
      "30. ▁Die ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁da ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁verz eihen ▁sie ▁sie ▁ihnen . [p = -13.37]\n",
      "Sanity check: log likehood of the best translation = -11.66\n"
     ]
    }
   ],
   "source": [
    "n_beams = 30\n",
    "english_sentence = 'Children begin by loving their parents; as they grow older they judge them; sometimes they forgive them.'\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimizer initialized...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tokens], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: log likehood of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: Children begin by loving their parents; as they grow older they judge them; sometimes they forgive them.\n",
      "Top 1 translations:\n",
      "1. ▁Kinder ▁beginnen ▁damit , ▁ihre ▁Eltern ▁zu ▁lieben ; ▁wenn ▁sie ▁älte r ▁werden , ▁beurteilen ▁sie ▁sie ; ▁ manchmal ▁vergeben ▁sie ▁sie ▁ihnen . [p = -11.85]\n",
      "Sanity check: log likehood of the best translation = -11.85\n"
     ]
    }
   ],
   "source": [
    "n_beams = 1\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimizer initialized...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tokens], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: log likehood of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: There is always something ridiculous about the emotions of people whom one has ceased to love.\n",
      "Top 30 translations:\n",
      "1. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -8.88]\n",
      "2. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.00]\n",
      "3. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.06]\n",
      "4. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.08]\n",
      "5. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -9.32]\n",
      "6. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Gefühl e ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.35]\n",
      "7. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -9.44]\n",
      "8. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Gefühl en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.44]\n",
      "9. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Gefühl en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.44]\n",
      "10. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -9.45]\n",
      "11. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -9.52]\n",
      "12. ▁Es ▁ist ▁immer ▁etwas ▁lä cherlich ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.59]\n",
      "13. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -9.60]\n",
      "14. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -9.60]\n",
      "15. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Gefühl e ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.65]\n",
      "16. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -9.71]\n",
      "17. ▁Es ▁ist ▁immer ▁etwas ▁lä cherlich ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -9.75]\n",
      "18. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Gefühl e ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -9.78]\n",
      "19. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -9.80]\n",
      "20. ▁Es ▁ist ▁immer ▁etwas ▁lä cherlich ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -9.84]\n",
      "21. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Gefühl e ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -9.94]\n",
      "22. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Gefühl en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -9.94]\n",
      "23. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Gefühl en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -9.99]\n",
      "24. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Gefühl e ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -10.00]\n",
      "25. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Gefühl en ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁zu ▁lieben . [p = -10.04]\n",
      "26. ▁Es ▁ist ▁immer ▁etwas ▁lä cherlich ▁über ▁die ▁Gefühl e ▁von ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -10.05]\n",
      "27. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Gefühl en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -10.06]\n",
      "28. ▁Es ▁gibt ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁zu ▁lieben . [p = -10.26]\n",
      "29. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁über ▁die ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁zu ▁lieben . [p = -10.27]\n",
      "30. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁der ▁Menschen , ▁die ▁man ▁auf gehört ▁hat ▁zu ▁lieben . [p = -10.33]\n",
      "Sanity check: log likehood of the best translation = -8.88\n"
     ]
    }
   ],
   "source": [
    "n_beams = 30\n",
    "english_sentence = \"There is always something ridiculous about the emotions of people whom one has ceased to love.\"\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimizer initialized...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tokens], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: log likehood of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: There is always something ridiculous about the emotions of people whom one has ceased to love.\n",
      "Top 1 translations:\n",
      "1. ▁Es ▁ist ▁immer ▁etwas ▁Lä cherlich es ▁an ▁den ▁Emotion en ▁von ▁Menschen , ▁die ▁man ▁nicht ▁mehr ▁lieben . [p = -9.60]\n",
      "Sanity check: log likehood of the best translation = -9.60\n"
     ]
    }
   ],
   "source": [
    "n_beams = 1\n",
    "english_sentence = \"There is always something ridiculous about the emotions of people whom one has ceased to love.\"\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimizer initialized...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tokens], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: log likehood of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: I count him braver who overcomes his desires than him who conquers his enemies; for the hardest victory is over self.\n",
      "Top 10 translations:\n",
      "1. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁der , ▁der ▁seine ▁Fein de ▁erobert ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -9.36]\n",
      "2. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁er , ▁der ▁seine ▁Fein de ▁erobert ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -9.37]\n",
      "3. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁der j en ige , ▁der ▁seine ▁Fein de ▁erobert ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -9.54]\n",
      "4. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et , ▁als ▁den , ▁der ▁seine ▁Fein de ▁erobert ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -9.68]\n",
      "5. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁er , ▁der ▁seine ▁Fein de ▁er ober t ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -10.36]\n",
      "6. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁der , ▁der ▁seine ▁Fein de ▁er ober t ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -10.46]\n",
      "7. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁der j en ige , ▁der ▁seine ▁Fein de ▁er ober t ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -10.73]\n",
      "8. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁der j en ige , ▁der ▁seine ▁Fein de ▁erobert ; ▁denn ▁der ▁schwer ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -10.74]\n",
      "9. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁der j en ige , ▁der ▁seine ▁Fein de ▁besieg t ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -10.75]\n",
      "10. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et , ▁als ▁den , ▁der ▁seine ▁Fein de ▁er ober t ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -10.77]\n",
      "Sanity check: log likehood of the best translation = -9.36\n"
     ]
    }
   ],
   "source": [
    "n_beams = 10\n",
    "english_sentence = \"I count him braver who overcomes his desires than him who conquers his enemies; for the hardest victory is over self.\"\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimizer initialized...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tokens], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: log likehood of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer initialized...\n",
      "English sentence: I count him braver who overcomes his desires than him who conquers his enemies; for the hardest victory is over self.\n",
      "Top 1 translations:\n",
      "1. ▁Ich ▁z ähle ▁ihn ▁mut iger , ▁der ▁seine ▁Wünsche ▁über wind et ▁als ▁der j en ige , ▁der ▁seine ▁Fein de ▁erobert ; ▁denn ▁der ▁ härte ste ▁Sieg ▁ist ▁über ▁sich ▁selbst . [p = -9.54]\n",
      "Sanity check: log likehood of the best translation = -9.54\n"
     ]
    }
   ],
   "source": [
    "n_beams = 1\n",
    "english_sentence = \"I count him braver who overcomes his desires than him who conquers his enemies; for the hardest victory is over self.\"\n",
    "english_tokens = tokenizer.tokenize(english_sentence)\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimizer initialized...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tokens], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: log likehood of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization starts...\n",
      "Score at step  0 = tensor([24.4894], grad_fn=<NegBackward>) max grad component =  lr =  99.0\n",
      "Score at step  1 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  59.4\n",
      "Score at step  2 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  35.64\n",
      "Score at step  3 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  21.384\n",
      "Score at step  4 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  12.8304\n",
      "Score at step  5 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.698239999999999\n",
      "Score at step  6 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.618943999999999\n",
      "Score at step  7 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.7713663999999993\n",
      "Score at step  8 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.6628198399999996\n",
      "Score at step  9 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.9976919039999997\n",
      "Score at step  10 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.5986151423999998\n",
      "Score at step  11 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.35916908543999987\n",
      "Score at step  12 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.21550145126399992\n",
      "Score at step  13 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.12930087075839994\n",
      "Score at step  14 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.07758052245503996\n",
      "Score at step  15 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.046548313473023975\n",
      "Score at step  16 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.027928988083814384\n",
      "Score at step  17 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.01675739285028863\n",
      "Score at step  18 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.010054435710173178\n",
      "Score at step  19 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.006032661426103906\n",
      "Score at step  20 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.0036195968556623436\n",
      "Score at step  21 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.0021717581133974062\n",
      "Score at step  22 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.0013030548680384437\n",
      "Score at step  23 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.0007818329208230662\n",
      "Score at step  24 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.0004690997524938397\n",
      "Score at step  25 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.0002814598514963038\n",
      "Score at step  26 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.00016887591089778228\n",
      "Score at step  27 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  0.00010132554653866937\n",
      "Score at step  28 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.079532792320162e-05\n",
      "Score at step  29 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.6477196753920966e-05\n",
      "Score at step  30 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.188631805235258e-05\n",
      "Score at step  31 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3131790831411547e-05\n",
      "Score at step  32 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.879074498846928e-06\n",
      "Score at step  33 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.727444699308157e-06\n",
      "Score at step  34 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.836466819584894e-06\n",
      "Score at step  35 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7018800917509363e-06\n",
      "Score at step  36 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0211280550505618e-06\n",
      "Score at step  37 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.12676833030337e-07\n",
      "Score at step  38 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.676060998182022e-07\n",
      "Score at step  39 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2056365989092132e-07\n",
      "Score at step  40 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3233819593455278e-07\n",
      "Score at step  41 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.940291756073167e-08\n",
      "Score at step  42 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.7641750536439e-08\n",
      "Score at step  43 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.8585050321863398e-08\n",
      "Score at step  44 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7151030193118038e-08\n",
      "Score at step  45 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0290618115870823e-08\n",
      "Score at step  46 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.1743708695224934e-09\n",
      "Score at step  47 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.704622521713496e-09\n",
      "Score at step  48 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2227735130280976e-09\n",
      "Score at step  49 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3336641078168584e-09\n",
      "Score at step  50 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.001984646901151e-10\n",
      "Score at step  51 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.80119078814069e-10\n",
      "Score at step  52 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.880714472884414e-10\n",
      "Score at step  53 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.728428683730648e-10\n",
      "Score at step  54 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0370572102383888e-10\n",
      "Score at step  55 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.222343261430333e-11\n",
      "Score at step  56 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.733405956858199e-11\n",
      "Score at step  57 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2400435741149196e-11\n",
      "Score at step  58 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3440261444689517e-11\n",
      "Score at step  59 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.064156866813709e-12\n",
      "Score at step  60 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.838494120088225e-12\n",
      "Score at step  61 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.9030964720529352e-12\n",
      "Score at step  62 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.741857883231761e-12\n",
      "Score at step  63 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0451147299390565e-12\n",
      "Score at step  64 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.270688379634339e-13\n",
      "Score at step  65 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.7624130277806035e-13\n",
      "Score at step  66 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.257447816668362e-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  67 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3544686900010173e-13\n",
      "Score at step  68 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.126812140006103e-14\n",
      "Score at step  69 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.876087284003662e-14\n",
      "Score at step  70 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.925652370402197e-14\n",
      "Score at step  71 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7553914222413183e-14\n",
      "Score at step  72 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.053234853344791e-14\n",
      "Score at step  73 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.319409120068745e-15\n",
      "Score at step  74 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.791645472041247e-15\n",
      "Score at step  75 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2749872832247482e-15\n",
      "Score at step  76 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3649923699348489e-15\n",
      "Score at step  77 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.189954219609093e-16\n",
      "Score at step  78 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.913972531765455e-16\n",
      "Score at step  79 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.948383519059273e-16\n",
      "Score at step  80 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7690301114355639e-16\n",
      "Score at step  81 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0614180668613382e-16\n",
      "Score at step  82 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.368508401168029e-17\n",
      "Score at step  83 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.821105040700817e-17\n",
      "Score at step  84 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2926630244204903e-17\n",
      "Score at step  85 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3755978146522941e-17\n",
      "Score at step  86 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.253586887913764e-18\n",
      "Score at step  87 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.952152132748258e-18\n",
      "Score at step  88 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.9712912796489548e-18\n",
      "Score at step  89 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.782774767789373e-18\n",
      "Score at step  90 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0696648606736238e-18\n",
      "Score at step  91 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.417989164041743e-19\n",
      "Score at step  92 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.8507934984250455e-19\n",
      "Score at step  93 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.310476099055027e-19\n",
      "Score at step  94 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.386285659433016e-19\n",
      "Score at step  95 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.317713956598096e-20\n",
      "Score at step  96 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.9906283739588574e-20\n",
      "Score at step  97 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.994377024375314e-20\n",
      "Score at step  98 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7966262146251886e-20\n",
      "Score at step  99 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0779757287751132e-20\n",
      "Score at step  100 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.467854372650679e-21\n",
      "Score at step  101 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.880712623590407e-21\n",
      "Score at step  102 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.328427574154244e-21\n",
      "Score at step  103 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3970565444925465e-21\n",
      "Score at step  104 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.382339266955279e-22\n",
      "Score at step  105 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.0294035601731675e-22\n",
      "Score at step  106 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.0176421361039005e-22\n",
      "Score at step  107 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8105852816623403e-22\n",
      "Score at step  108 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0863511689974042e-22\n",
      "Score at step  109 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.518107013984425e-23\n",
      "Score at step  110 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.910864208390655e-23\n",
      "Score at step  111 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.346518525034393e-23\n",
      "Score at step  112 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4079111150206355e-23\n",
      "Score at step  113 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.447466690123813e-24\n",
      "Score at step  114 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.068480014074288e-24\n",
      "Score at step  115 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.0410880084445728e-24\n",
      "Score at step  116 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8246528050667436e-24\n",
      "Score at step  117 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0947916830400462e-24\n",
      "Score at step  118 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.568750098240277e-25\n",
      "Score at step  119 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.941250058944166e-25\n",
      "Score at step  120 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.3647500353664997e-25\n",
      "Score at step  121 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4188500212198997e-25\n",
      "Score at step  122 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.513100127319398e-26\n",
      "Score at step  123 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.107860076391639e-26\n",
      "Score at step  124 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.064716045834983e-26\n",
      "Score at step  125 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.83882962750099e-26\n",
      "Score at step  126 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.103297776500594e-26\n",
      "Score at step  127 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.619786659003563e-27\n",
      "Score at step  128 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.9718719954021375e-27\n",
      "Score at step  129 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.3831231972412826e-27\n",
      "Score at step  130 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4298739183447696e-27\n",
      "Score at step  131 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.579243510068617e-28\n",
      "Score at step  132 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.14754610604117e-28\n",
      "Score at step  133 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.0885276636247017e-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  134 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.853116598174821e-28\n",
      "Score at step  135 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1118699589048926e-28\n",
      "Score at step  136 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.671219753429356e-29\n",
      "Score at step  137 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.0027318520576135e-29\n",
      "Score at step  138 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.401639111234568e-29\n",
      "Score at step  139 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4409834667407408e-29\n",
      "Score at step  140 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.645900800444445e-30\n",
      "Score at step  141 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.187540480266667e-30\n",
      "Score at step  142 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.11252428816e-30\n",
      "Score at step  143 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.867514572896e-30\n",
      "Score at step  144 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1205087437376e-30\n",
      "Score at step  145 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.723052462425599e-31\n",
      "Score at step  146 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.0338314774553595e-31\n",
      "Score at step  147 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.4202988864732154e-31\n",
      "Score at step  148 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4521793318839292e-31\n",
      "Score at step  149 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.713075991303575e-32\n",
      "Score at step  150 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.227845594782145e-32\n",
      "Score at step  151 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.1367073568692866e-32\n",
      "Score at step  152 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.882024414121572e-32\n",
      "Score at step  153 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1292146484729432e-32\n",
      "Score at step  154 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.775287890837659e-33\n",
      "Score at step  155 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.0651727345025953e-33\n",
      "Score at step  156 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.439103640701557e-33\n",
      "Score at step  157 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4634621844209341e-33\n",
      "Score at step  158 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.780773106525605e-34\n",
      "Score at step  159 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.2684638639153624e-34\n",
      "Score at step  160 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.161078318349217e-34\n",
      "Score at step  161 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8966469910095302e-34\n",
      "Score at step  162 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1379881946057182e-34\n",
      "Score at step  163 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.827929167634309e-35\n",
      "Score at step  164 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.096757500580585e-35\n",
      "Score at step  165 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.458054500348351e-35\n",
      "Score at step  166 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4748327002090104e-35\n",
      "Score at step  167 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.848996201254062e-36\n",
      "Score at step  168 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.3093977207524374e-36\n",
      "Score at step  169 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.185638632451462e-36\n",
      "Score at step  170 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9113831794708772e-36\n",
      "Score at step  171 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1468299076825262e-36\n",
      "Score at step  172 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.880979446095157e-37\n",
      "Score at step  173 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.128587667657094e-37\n",
      "Score at step  174 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.4771526005942564e-37\n",
      "Score at step  175 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4862915603565537e-37\n",
      "Score at step  176 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.917749362139322e-38\n",
      "Score at step  177 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.350649617283593e-38\n",
      "Score at step  178 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.210389770370156e-38\n",
      "Score at step  179 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9262338622220935e-38\n",
      "Score at step  180 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.155740317333256e-38\n",
      "Score at step  181 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.934441903999535e-39\n",
      "Score at step  182 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.160665142399721e-39\n",
      "Score at step  183 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.4963990854398326e-39\n",
      "Score at step  184 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4978394512638996e-39\n",
      "Score at step  185 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.987036707583397e-40\n",
      "Score at step  186 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.392222024550038e-40\n",
      "Score at step  187 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.235333214730023e-40\n",
      "Score at step  188 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9411999288380138e-40\n",
      "Score at step  189 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1647199573028083e-40\n",
      "Score at step  190 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.98831974381685e-41\n",
      "Score at step  191 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.1929918462901096e-41\n",
      "Score at step  192 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.515795107774066e-41\n",
      "Score at step  193 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5094770646644394e-41\n",
      "Score at step  194 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.056862387986636e-42\n",
      "Score at step  195 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.434117432791981e-42\n",
      "Score at step  196 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.260470459675189e-42\n",
      "Score at step  197 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9562822758051133e-42\n",
      "Score at step  198 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.173769365483068e-42\n",
      "Score at step  199 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.042616192898407e-43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  200 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.225569715739044e-43\n",
      "Score at step  201 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.535341829443426e-43\n",
      "Score at step  202 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5212050976660558e-43\n",
      "Score at step  203 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.127230585996333e-44\n",
      "Score at step  204 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.476338351597799e-44\n",
      "Score at step  205 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.2858030109586797e-44\n",
      "Score at step  206 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9714818065752077e-44\n",
      "Score at step  207 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1828890839451247e-44\n",
      "Score at step  208 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.097334503670747e-45\n",
      "Score at step  209 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.258400702202448e-45\n",
      "Score at step  210 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.5550404213214687e-45\n",
      "Score at step  211 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5330242527928812e-45\n",
      "Score at step  212 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.198145516757287e-46\n",
      "Score at step  213 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.518887310054372e-46\n",
      "Score at step  214 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.3113323860326232e-46\n",
      "Score at step  215 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9867994316195737e-46\n",
      "Score at step  216 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1920796589717442e-46\n",
      "Score at step  217 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.152477953830465e-47\n",
      "Score at step  218 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.2914867722982787e-47\n",
      "Score at step  219 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.5748920633789673e-47\n",
      "Score at step  220 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5449352380273802e-47\n",
      "Score at step  221 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.269611428164281e-48\n",
      "Score at step  222 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.561766856898568e-48\n",
      "Score at step  223 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.3370601141391407e-48\n",
      "Score at step  224 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.0022360684834844e-48\n",
      "Score at step  225 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2013416410900906e-48\n",
      "Score at step  226 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.208049846540544e-49\n",
      "Score at step  227 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.3248299079243266e-49\n",
      "Score at step  228 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.594897944754596e-49\n",
      "Score at step  229 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5569387668527576e-49\n",
      "Score at step  230 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.341632601116546e-50\n",
      "Score at step  231 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.604979560669927e-50\n",
      "Score at step  232 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.3629877364019564e-50\n",
      "Score at step  233 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.0177926418411739e-50\n",
      "Score at step  234 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2106755851047043e-50\n",
      "Score at step  235 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.264053510628226e-51\n",
      "Score at step  236 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.3584321063769353e-51\n",
      "Score at step  237 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.6150592638261612e-51\n",
      "Score at step  238 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5690355582956967e-51\n",
      "Score at step  239 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.41421334977418e-52\n",
      "Score at step  240 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.648528009864508e-52\n",
      "Score at step  241 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.3891168059187045e-52\n",
      "Score at step  242 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.0334700835512227e-52\n",
      "Score at step  243 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2200820501307336e-52\n",
      "Score at step  244 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.320492300784401e-53\n",
      "Score at step  245 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.392295380470641e-53\n",
      "Score at step  246 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.6353772282823845e-53\n",
      "Score at step  247 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5812263369694307e-53\n",
      "Score at step  248 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.487358021816584e-54\n",
      "Score at step  249 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.69241481308995e-54\n",
      "Score at step  250 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.41544888785397e-54\n",
      "Score at step  251 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.049269332712382e-54\n",
      "Score at step  252 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.229561599627429e-54\n",
      "Score at step  253 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.377369597764574e-55\n",
      "Score at step  254 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.426421758658744e-55\n",
      "Score at step  255 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.6558530551952465e-55\n",
      "Score at step  256 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5935118331171479e-55\n",
      "Score at step  257 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.561070998702888e-56\n",
      "Score at step  258 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.736642599221733e-56\n",
      "Score at step  259 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.4419855595330397e-56\n",
      "Score at step  260 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.065191335719824e-56\n",
      "Score at step  261 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2391148014318942e-56\n",
      "Score at step  262 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.434688808591365e-57\n",
      "Score at step  263 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.4608132851548185e-57\n",
      "Score at step  264 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.676487971092891e-57\n",
      "Score at step  265 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.6058927826557346e-57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  266 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.635356695934406e-58\n",
      "Score at step  267 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.7812140175606435e-58\n",
      "Score at step  268 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.468728410536386e-58\n",
      "Score at step  269 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.0812370463218313e-58\n",
      "Score at step  270 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2487422277930988e-58\n",
      "Score at step  271 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.492453366758592e-59\n",
      "Score at step  272 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.495472020055155e-59\n",
      "Score at step  273 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.697283212033093e-59\n",
      "Score at step  274 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.6183699272198558e-59\n",
      "Score at step  275 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.710219563319135e-60\n",
      "Score at step  276 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.826131737991481e-60\n",
      "Score at step  277 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.4956790427948885e-60\n",
      "Score at step  278 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.097407425676933e-60\n",
      "Score at step  279 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2584444554061598e-60\n",
      "Score at step  280 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.550666732436959e-61\n",
      "Score at step  281 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.5304000394621755e-61\n",
      "Score at step  282 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.718240023677305e-61\n",
      "Score at step  283 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.630944014206383e-61\n",
      "Score at step  284 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.785664085238298e-62\n",
      "Score at step  285 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.871398451142979e-62\n",
      "Score at step  286 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.522839070685787e-62\n",
      "Score at step  287 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.113703442411472e-62\n",
      "Score at step  288 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2682220654468831e-62\n",
      "Score at step  289 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.609332392681298e-63\n",
      "Score at step  290 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.5655994356087784e-63\n",
      "Score at step  291 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.739359661365267e-63\n",
      "Score at step  292 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.64361579681916e-63\n",
      "Score at step  293 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.86169478091496e-64\n",
      "Score at step  294 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.917016868548976e-64\n",
      "Score at step  295 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.5502101211293856e-64\n",
      "Score at step  296 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.130126072677631e-64\n",
      "Score at step  297 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2780756436065786e-64\n",
      "Score at step  298 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.668453861639471e-65\n",
      "Score at step  299 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.601072316983682e-65\n",
      "Score at step  300 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.760643390190209e-65\n",
      "Score at step  301 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.6563860341141255e-65\n",
      "Score at step  302 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.938316204684752e-66\n",
      "Score at step  303 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.962989722810851e-66\n",
      "Score at step  304 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.5777938336865106e-66\n",
      "Score at step  305 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.1466763002119063e-66\n",
      "Score at step  306 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2880057801271437e-66\n",
      "Score at step  307 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.728034680762862e-67\n",
      "Score at step  308 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.636820808457717e-67\n",
      "Score at step  309 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.7820924850746304e-67\n",
      "Score at step  310 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.6692554910447783e-67\n",
      "Score at step  311 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0015532946268669e-67\n",
      "Score at step  312 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.0093197677612015e-68\n",
      "Score at step  313 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.6055918606567207e-68\n",
      "Score at step  314 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.163355116394032e-68\n",
      "Score at step  315 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.2980130698364192e-68\n",
      "Score at step  316 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.788078419018515e-69\n",
      "Score at step  317 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.672847051411109e-69\n",
      "Score at step  318 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.8037082308466653e-69\n",
      "Score at step  319 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.6822249385079992e-69\n",
      "Score at step  320 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0093349631047995e-69\n",
      "Score at step  321 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.056009778628797e-70\n",
      "Score at step  322 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.633605867177278e-70\n",
      "Score at step  323 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.180163520306367e-70\n",
      "Score at step  324 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3080981121838202e-70\n",
      "Score at step  325 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.848588673102921e-71\n",
      "Score at step  326 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.709153203861752e-71\n",
      "Score at step  327 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.8254919223170513e-71\n",
      "Score at step  328 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.6952951533902307e-71\n",
      "Score at step  329 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0171770920341384e-71\n",
      "Score at step  330 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.10306255220483e-72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  331 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.661837531322898e-72\n",
      "Score at step  332 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.1971025187937385e-72\n",
      "Score at step  333 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.318261511276243e-72\n",
      "Score at step  334 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.909569067657458e-73\n",
      "Score at step  335 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.7457414405944743e-73\n",
      "Score at step  336 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.8474448643566843e-73\n",
      "Score at step  337 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7084669186140106e-73\n",
      "Score at step  338 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0250801511684063e-73\n",
      "Score at step  339 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.150480907010438e-74\n",
      "Score at step  340 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.690288544206262e-74\n",
      "Score at step  341 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.214173126523757e-74\n",
      "Score at step  342 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3285038759142543e-74\n",
      "Score at step  343 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.971023255485526e-75\n",
      "Score at step  344 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.782613953291315e-75\n",
      "Score at step  345 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.869568371974789e-75\n",
      "Score at step  346 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7217410231848733e-75\n",
      "Score at step  347 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.033044613910924e-75\n",
      "Score at step  348 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.198267683465544e-76\n",
      "Score at step  349 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.718960610079326e-76\n",
      "Score at step  350 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2313763660475958e-76\n",
      "Score at step  351 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3388258196285573e-76\n",
      "Score at step  352 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.032954917771343e-77\n",
      "Score at step  353 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.8197729506628054e-77\n",
      "Score at step  354 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.891863770397683e-77\n",
      "Score at step  355 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.73511826223861e-77\n",
      "Score at step  356 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0410709573431659e-77\n",
      "Score at step  357 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.246425744058995e-78\n",
      "Score at step  358 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.747855446435397e-78\n",
      "Score at step  359 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.248713267861238e-78\n",
      "Score at step  360 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3492279607167426e-78\n",
      "Score at step  361 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.095367764300455e-79\n",
      "Score at step  362 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.857220658580273e-79\n",
      "Score at step  363 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.914332395148164e-79\n",
      "Score at step  364 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7485994370888983e-79\n",
      "Score at step  365 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0491596622533389e-79\n",
      "Score at step  366 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.294957973520033e-80\n",
      "Score at step  367 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.776974784112019e-80\n",
      "Score at step  368 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2661848704672114e-80\n",
      "Score at step  369 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3597109222803267e-80\n",
      "Score at step  370 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.15826553368196e-81\n",
      "Score at step  371 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.894959320209176e-81\n",
      "Score at step  372 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.9369755921255054e-81\n",
      "Score at step  373 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.762185355275303e-81\n",
      "Score at step  374 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0573112131651817e-81\n",
      "Score at step  375 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.34386727899109e-82\n",
      "Score at step  376 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.806320367394654e-82\n",
      "Score at step  377 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.2837922204367922e-82\n",
      "Score at step  378 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3702753322620754e-82\n",
      "Score at step  379 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.221651993572452e-83\n",
      "Score at step  380 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.932991196143471e-83\n",
      "Score at step  381 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.9597947176860826e-83\n",
      "Score at step  382 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.7758768306116497e-83\n",
      "Score at step  383 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0655260983669898e-83\n",
      "Score at step  384 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.393156590201938e-84\n",
      "Score at step  385 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.8358939541211624e-84\n",
      "Score at step  386 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.301536372472697e-84\n",
      "Score at step  387 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3809218234836183e-84\n",
      "Score at step  388 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.28553094090171e-85\n",
      "Score at step  389 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.9713185645410253e-85\n",
      "Score at step  390 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.982791138724615e-85\n",
      "Score at step  391 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.789674683234769e-85\n",
      "Score at step  392 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0738048099408614e-85\n",
      "Score at step  393 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.442828859645168e-86\n",
      "Score at step  394 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.865697315787101e-86\n",
      "Score at step  395 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.3194183894722603e-86\n",
      "Score at step  396 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.3916510336833561e-86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  397 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.349906202100137e-87\n",
      "Score at step  398 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.009943721260082e-87\n",
      "Score at step  399 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.0059662327560493e-87\n",
      "Score at step  400 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8035797396536294e-87\n",
      "Score at step  401 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.0821478437921776e-87\n",
      "Score at step  402 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.492887062753065e-88\n",
      "Score at step  403 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.895732237651839e-88\n",
      "Score at step  404 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.3374393425911033e-88\n",
      "Score at step  405 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.402463605554662e-88\n",
      "Score at step  406 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.414781633327971e-89\n",
      "Score at step  407 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.048868979996782e-89\n",
      "Score at step  408 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.0293213879980693e-89\n",
      "Score at step  409 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8175928327988417e-89\n",
      "Score at step  410 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.090555699679305e-89\n",
      "Score at step  411 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.54333419807583e-90\n",
      "Score at step  412 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.926000518845498e-90\n",
      "Score at step  413 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.3556003113072986e-90\n",
      "Score at step  414 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4133601867843791e-90\n",
      "Score at step  415 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.480161120706275e-91\n",
      "Score at step  416 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.088096672423765e-91\n",
      "Score at step  417 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.0528580034542586e-91\n",
      "Score at step  418 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8317148020725552e-91\n",
      "Score at step  419 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.099028881243533e-91\n",
      "Score at step  420 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.594173287461197e-92\n",
      "Score at step  421 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.956503972476718e-92\n",
      "Score at step  422 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.373902383486031e-92\n",
      "Score at step  423 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4243414300916184e-92\n",
      "Score at step  424 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.54604858054971e-93\n",
      "Score at step  425 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.127629148329826e-93\n",
      "Score at step  426 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.0765774889978955e-93\n",
      "Score at step  427 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8459464933987372e-93\n",
      "Score at step  428 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1075678960392423e-93\n",
      "Score at step  429 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.645407376235454e-94\n",
      "Score at step  430 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.987244425741272e-94\n",
      "Score at step  431 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.392346655444763e-94\n",
      "Score at step  432 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4354079932668578e-94\n",
      "Score at step  433 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.612447959601146e-95\n",
      "Score at step  434 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.167468775760688e-95\n",
      "Score at step  435 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.1004812654564125e-95\n",
      "Score at step  436 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8602887592738474e-95\n",
      "Score at step  437 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1161732555643083e-95\n",
      "Score at step  438 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.69703953338585e-96\n",
      "Score at step  439 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.01822372003151e-96\n",
      "Score at step  440 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.410934232018906e-96\n",
      "Score at step  441 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4465605392113436e-96\n",
      "Score at step  442 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.679363235268061e-97\n",
      "Score at step  443 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.207617941160836e-97\n",
      "Score at step  444 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.1245707646965015e-97\n",
      "Score at step  445 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.874742458817901e-97\n",
      "Score at step  446 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1248454752907405e-97\n",
      "Score at step  447 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.749072851744443e-98\n",
      "Score at step  448 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.049443711046666e-98\n",
      "Score at step  449 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.4296662266279993e-98\n",
      "Score at step  450 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4577997359767995e-98\n",
      "Score at step  451 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.746798415860796e-99\n",
      "Score at step  452 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.248079049516477e-99\n",
      "Score at step  453 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.1488474297098865e-99\n",
      "Score at step  454 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.8893084578259316e-99\n",
      "Score at step  455 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.133585074695559e-99\n",
      "Score at step  456 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.801510448173354e-100\n",
      "Score at step  457 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.080906268904012e-100\n",
      "Score at step  458 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.4485437613424074e-100\n",
      "Score at step  459 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4691262568054445e-100\n",
      "Score at step  460 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.814757540832667e-101\n",
      "Score at step  461 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.2888545244995996e-101\n",
      "Score at step  462 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.17331271469976e-101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  463 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9039876288198557e-101\n",
      "Score at step  464 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1423925772919134e-101\n",
      "Score at step  465 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.8543554637514805e-102\n",
      "Score at step  466 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.1126132782508884e-102\n",
      "Score at step  467 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.467567966950533e-102\n",
      "Score at step  468 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4805407801703196e-102\n",
      "Score at step  469 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.883244681021917e-103\n",
      "Score at step  470 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.32994680861315e-103\n",
      "Score at step  471 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.19796808516789e-103\n",
      "Score at step  472 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.918780851100734e-103\n",
      "Score at step  473 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1512685106604403e-103\n",
      "Score at step  474 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.907611063962642e-104\n",
      "Score at step  475 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.144566638377585e-104\n",
      "Score at step  476 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.4867399830265508e-104\n",
      "Score at step  477 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.4920439898159305e-104\n",
      "Score at step  478 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  8.952263938895583e-105\n",
      "Score at step  479 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.37135836333735e-105\n",
      "Score at step  480 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.2228150180024095e-105\n",
      "Score at step  481 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.9336890108014457e-105\n",
      "Score at step  482 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1602134064808673e-105\n",
      "Score at step  483 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  6.961280438885203e-106\n",
      "Score at step  484 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.1767682633311215e-106\n",
      "Score at step  485 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.5060609579986726e-106\n",
      "Score at step  486 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5036365747992035e-106\n",
      "Score at step  487 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.021819448795221e-107\n",
      "Score at step  488 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.413091669277132e-107\n",
      "Score at step  489 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.247855001566279e-107\n",
      "Score at step  490 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.948713000939767e-107\n",
      "Score at step  491 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.1692278005638602e-107\n",
      "Score at step  492 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  7.015366803383161e-108\n",
      "Score at step  493 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  4.209220082029897e-108\n",
      "Score at step  494 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  2.525532049217938e-108\n",
      "Score at step  495 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.5153192295307626e-108\n",
      "Score at step  496 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  9.091915377184575e-109\n",
      "Score at step  497 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  5.455149226310745e-109\n",
      "Score at step  498 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  3.273089535786447e-109\n",
      "Score at step  499 = tensor([24.4894], grad_fn=<NegBackward>) max grad component = tensor(0.0002) lr =  1.963853721471868e-109\n",
      "Max(Y) =  tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "['▁Zwar', 'staatlich', 'staatlich', 'örtlich', '▁Grenz', '▁Grenz', '▁Grenz', '▁Sollte', '▁Sollte', '▁Gas', '▁Gas', '▁Gas', '▁Bade', '▁Jede', '▁Plan', '▁Grenz', '▁Jede', '▁Denn', '▁Einzel', '▁Einzel', '▁Bade', '▁Grenz', '▁Grenz', '?']\n"
     ]
    }
   ],
   "source": [
    "english_sentence = 'I think, that machine translation is a very interesting subject.'\n",
    "english_tok = tokenizer.tokenize(english_sentence)\n",
    "optimizer = ContinuousOptimizer(english_sentence)\n",
    "print('Optimization starts...')\n",
    "res = optimizer.optimize()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy initialization with one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization starts...\n",
      "Score at step  0 = tensor([0.4511], grad_fn=<NegBackward>) max grad component =  lr =  99.0\n",
      "Score at step  1 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  59.4\n",
      "Score at step  2 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  35.64\n",
      "Score at step  3 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  21.384\n",
      "Score at step  4 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  12.8304\n",
      "Score at step  5 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.698239999999999\n",
      "Score at step  6 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.618943999999999\n",
      "Score at step  7 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.7713663999999993\n",
      "Score at step  8 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.6628198399999996\n",
      "Score at step  9 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.9976919039999997\n",
      "Score at step  10 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.5986151423999998\n",
      "Score at step  11 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.35916908543999987\n",
      "Score at step  12 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.21550145126399992\n",
      "Score at step  13 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.12930087075839994\n",
      "Score at step  14 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.07758052245503996\n",
      "Score at step  15 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.046548313473023975\n",
      "Score at step  16 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.027928988083814384\n",
      "Score at step  17 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.01675739285028863\n",
      "Score at step  18 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.010054435710173178\n",
      "Score at step  19 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.006032661426103906\n",
      "Score at step  20 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.0036195968556623436\n",
      "Score at step  21 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.0021717581133974062\n",
      "Score at step  22 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.0013030548680384437\n",
      "Score at step  23 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.0007818329208230662\n",
      "Score at step  24 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.0004690997524938397\n",
      "Score at step  25 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.0002814598514963038\n",
      "Score at step  26 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.00016887591089778228\n",
      "Score at step  27 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  0.00010132554653866937\n",
      "Score at step  28 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.079532792320162e-05\n",
      "Score at step  29 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.6477196753920966e-05\n",
      "Score at step  30 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.188631805235258e-05\n",
      "Score at step  31 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3131790831411547e-05\n",
      "Score at step  32 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.879074498846928e-06\n",
      "Score at step  33 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.727444699308157e-06\n",
      "Score at step  34 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.836466819584894e-06\n",
      "Score at step  35 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7018800917509363e-06\n",
      "Score at step  36 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0211280550505618e-06\n",
      "Score at step  37 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.12676833030337e-07\n",
      "Score at step  38 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.676060998182022e-07\n",
      "Score at step  39 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2056365989092132e-07\n",
      "Score at step  40 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3233819593455278e-07\n",
      "Score at step  41 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.940291756073167e-08\n",
      "Score at step  42 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.7641750536439e-08\n",
      "Score at step  43 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.8585050321863398e-08\n",
      "Score at step  44 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7151030193118038e-08\n",
      "Score at step  45 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0290618115870823e-08\n",
      "Score at step  46 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.1743708695224934e-09\n",
      "Score at step  47 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.704622521713496e-09\n",
      "Score at step  48 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2227735130280976e-09\n",
      "Score at step  49 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3336641078168584e-09\n",
      "Score at step  50 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.001984646901151e-10\n",
      "Score at step  51 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.80119078814069e-10\n",
      "Score at step  52 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.880714472884414e-10\n",
      "Score at step  53 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.728428683730648e-10\n",
      "Score at step  54 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0370572102383888e-10\n",
      "Score at step  55 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.222343261430333e-11\n",
      "Score at step  56 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.733405956858199e-11\n",
      "Score at step  57 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2400435741149196e-11\n",
      "Score at step  58 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3440261444689517e-11\n",
      "Score at step  59 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.064156866813709e-12\n",
      "Score at step  60 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.838494120088225e-12\n",
      "Score at step  61 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.9030964720529352e-12\n",
      "Score at step  62 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.741857883231761e-12\n",
      "Score at step  63 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0451147299390565e-12\n",
      "Score at step  64 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.270688379634339e-13\n",
      "Score at step  65 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.7624130277806035e-13\n",
      "Score at step  66 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.257447816668362e-13\n",
      "Score at step  67 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3544686900010173e-13\n",
      "Score at step  68 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.126812140006103e-14\n",
      "Score at step  69 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.876087284003662e-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  70 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.925652370402197e-14\n",
      "Score at step  71 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7553914222413183e-14\n",
      "Score at step  72 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.053234853344791e-14\n",
      "Score at step  73 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.319409120068745e-15\n",
      "Score at step  74 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.791645472041247e-15\n",
      "Score at step  75 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2749872832247482e-15\n",
      "Score at step  76 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3649923699348489e-15\n",
      "Score at step  77 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.189954219609093e-16\n",
      "Score at step  78 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.913972531765455e-16\n",
      "Score at step  79 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.948383519059273e-16\n",
      "Score at step  80 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7690301114355639e-16\n",
      "Score at step  81 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0614180668613382e-16\n",
      "Score at step  82 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.368508401168029e-17\n",
      "Score at step  83 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.821105040700817e-17\n",
      "Score at step  84 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2926630244204903e-17\n",
      "Score at step  85 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3755978146522941e-17\n",
      "Score at step  86 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.253586887913764e-18\n",
      "Score at step  87 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.952152132748258e-18\n",
      "Score at step  88 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.9712912796489548e-18\n",
      "Score at step  89 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.782774767789373e-18\n",
      "Score at step  90 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0696648606736238e-18\n",
      "Score at step  91 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.417989164041743e-19\n",
      "Score at step  92 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.8507934984250455e-19\n",
      "Score at step  93 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.310476099055027e-19\n",
      "Score at step  94 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.386285659433016e-19\n",
      "Score at step  95 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.317713956598096e-20\n",
      "Score at step  96 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.9906283739588574e-20\n",
      "Score at step  97 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.994377024375314e-20\n",
      "Score at step  98 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7966262146251886e-20\n",
      "Score at step  99 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0779757287751132e-20\n",
      "Score at step  100 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.467854372650679e-21\n",
      "Score at step  101 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.880712623590407e-21\n",
      "Score at step  102 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.328427574154244e-21\n",
      "Score at step  103 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3970565444925465e-21\n",
      "Score at step  104 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.382339266955279e-22\n",
      "Score at step  105 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.0294035601731675e-22\n",
      "Score at step  106 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.0176421361039005e-22\n",
      "Score at step  107 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8105852816623403e-22\n",
      "Score at step  108 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0863511689974042e-22\n",
      "Score at step  109 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.518107013984425e-23\n",
      "Score at step  110 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.910864208390655e-23\n",
      "Score at step  111 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.346518525034393e-23\n",
      "Score at step  112 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4079111150206355e-23\n",
      "Score at step  113 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.447466690123813e-24\n",
      "Score at step  114 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.068480014074288e-24\n",
      "Score at step  115 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.0410880084445728e-24\n",
      "Score at step  116 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8246528050667436e-24\n",
      "Score at step  117 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0947916830400462e-24\n",
      "Score at step  118 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.568750098240277e-25\n",
      "Score at step  119 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.941250058944166e-25\n",
      "Score at step  120 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.3647500353664997e-25\n",
      "Score at step  121 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4188500212198997e-25\n",
      "Score at step  122 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.513100127319398e-26\n",
      "Score at step  123 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.107860076391639e-26\n",
      "Score at step  124 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.064716045834983e-26\n",
      "Score at step  125 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.83882962750099e-26\n",
      "Score at step  126 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.103297776500594e-26\n",
      "Score at step  127 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.619786659003563e-27\n",
      "Score at step  128 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.9718719954021375e-27\n",
      "Score at step  129 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.3831231972412826e-27\n",
      "Score at step  130 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4298739183447696e-27\n",
      "Score at step  131 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.579243510068617e-28\n",
      "Score at step  132 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.14754610604117e-28\n",
      "Score at step  133 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.0885276636247017e-28\n",
      "Score at step  134 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.853116598174821e-28\n",
      "Score at step  135 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1118699589048926e-28\n",
      "Score at step  136 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.671219753429356e-29\n",
      "Score at step  137 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.0027318520576135e-29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  138 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.401639111234568e-29\n",
      "Score at step  139 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4409834667407408e-29\n",
      "Score at step  140 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.645900800444445e-30\n",
      "Score at step  141 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.187540480266667e-30\n",
      "Score at step  142 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.11252428816e-30\n",
      "Score at step  143 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.867514572896e-30\n",
      "Score at step  144 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1205087437376e-30\n",
      "Score at step  145 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.723052462425599e-31\n",
      "Score at step  146 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.0338314774553595e-31\n",
      "Score at step  147 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.4202988864732154e-31\n",
      "Score at step  148 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4521793318839292e-31\n",
      "Score at step  149 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.713075991303575e-32\n",
      "Score at step  150 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.227845594782145e-32\n",
      "Score at step  151 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.1367073568692866e-32\n",
      "Score at step  152 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.882024414121572e-32\n",
      "Score at step  153 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1292146484729432e-32\n",
      "Score at step  154 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.775287890837659e-33\n",
      "Score at step  155 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.0651727345025953e-33\n",
      "Score at step  156 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.439103640701557e-33\n",
      "Score at step  157 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4634621844209341e-33\n",
      "Score at step  158 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.780773106525605e-34\n",
      "Score at step  159 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.2684638639153624e-34\n",
      "Score at step  160 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.161078318349217e-34\n",
      "Score at step  161 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8966469910095302e-34\n",
      "Score at step  162 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1379881946057182e-34\n",
      "Score at step  163 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.827929167634309e-35\n",
      "Score at step  164 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.096757500580585e-35\n",
      "Score at step  165 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.458054500348351e-35\n",
      "Score at step  166 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4748327002090104e-35\n",
      "Score at step  167 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.848996201254062e-36\n",
      "Score at step  168 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.3093977207524374e-36\n",
      "Score at step  169 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.185638632451462e-36\n",
      "Score at step  170 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9113831794708772e-36\n",
      "Score at step  171 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1468299076825262e-36\n",
      "Score at step  172 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.880979446095157e-37\n",
      "Score at step  173 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.128587667657094e-37\n",
      "Score at step  174 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.4771526005942564e-37\n",
      "Score at step  175 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4862915603565537e-37\n",
      "Score at step  176 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.917749362139322e-38\n",
      "Score at step  177 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.350649617283593e-38\n",
      "Score at step  178 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.210389770370156e-38\n",
      "Score at step  179 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9262338622220935e-38\n",
      "Score at step  180 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.155740317333256e-38\n",
      "Score at step  181 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.934441903999535e-39\n",
      "Score at step  182 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.160665142399721e-39\n",
      "Score at step  183 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.4963990854398326e-39\n",
      "Score at step  184 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4978394512638996e-39\n",
      "Score at step  185 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.987036707583397e-40\n",
      "Score at step  186 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.392222024550038e-40\n",
      "Score at step  187 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.235333214730023e-40\n",
      "Score at step  188 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9411999288380138e-40\n",
      "Score at step  189 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1647199573028083e-40\n",
      "Score at step  190 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.98831974381685e-41\n",
      "Score at step  191 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.1929918462901096e-41\n",
      "Score at step  192 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.515795107774066e-41\n",
      "Score at step  193 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5094770646644394e-41\n",
      "Score at step  194 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.056862387986636e-42\n",
      "Score at step  195 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.434117432791981e-42\n",
      "Score at step  196 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.260470459675189e-42\n",
      "Score at step  197 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9562822758051133e-42\n",
      "Score at step  198 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.173769365483068e-42\n",
      "Score at step  199 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.042616192898407e-43\n",
      "Score at step  200 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.225569715739044e-43\n",
      "Score at step  201 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.535341829443426e-43\n",
      "Score at step  202 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5212050976660558e-43\n",
      "Score at step  203 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.127230585996333e-44\n",
      "Score at step  204 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.476338351597799e-44\n",
      "Score at step  205 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.2858030109586797e-44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  206 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9714818065752077e-44\n",
      "Score at step  207 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1828890839451247e-44\n",
      "Score at step  208 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.097334503670747e-45\n",
      "Score at step  209 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.258400702202448e-45\n",
      "Score at step  210 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.5550404213214687e-45\n",
      "Score at step  211 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5330242527928812e-45\n",
      "Score at step  212 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.198145516757287e-46\n",
      "Score at step  213 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.518887310054372e-46\n",
      "Score at step  214 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.3113323860326232e-46\n",
      "Score at step  215 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9867994316195737e-46\n",
      "Score at step  216 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1920796589717442e-46\n",
      "Score at step  217 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.152477953830465e-47\n",
      "Score at step  218 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.2914867722982787e-47\n",
      "Score at step  219 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.5748920633789673e-47\n",
      "Score at step  220 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5449352380273802e-47\n",
      "Score at step  221 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.269611428164281e-48\n",
      "Score at step  222 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.561766856898568e-48\n",
      "Score at step  223 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.3370601141391407e-48\n",
      "Score at step  224 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.0022360684834844e-48\n",
      "Score at step  225 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2013416410900906e-48\n",
      "Score at step  226 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.208049846540544e-49\n",
      "Score at step  227 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.3248299079243266e-49\n",
      "Score at step  228 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.594897944754596e-49\n",
      "Score at step  229 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5569387668527576e-49\n",
      "Score at step  230 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.341632601116546e-50\n",
      "Score at step  231 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.604979560669927e-50\n",
      "Score at step  232 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.3629877364019564e-50\n",
      "Score at step  233 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.0177926418411739e-50\n",
      "Score at step  234 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2106755851047043e-50\n",
      "Score at step  235 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.264053510628226e-51\n",
      "Score at step  236 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.3584321063769353e-51\n",
      "Score at step  237 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.6150592638261612e-51\n",
      "Score at step  238 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5690355582956967e-51\n",
      "Score at step  239 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.41421334977418e-52\n",
      "Score at step  240 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.648528009864508e-52\n",
      "Score at step  241 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.3891168059187045e-52\n",
      "Score at step  242 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.0334700835512227e-52\n",
      "Score at step  243 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2200820501307336e-52\n",
      "Score at step  244 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.320492300784401e-53\n",
      "Score at step  245 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.392295380470641e-53\n",
      "Score at step  246 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.6353772282823845e-53\n",
      "Score at step  247 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5812263369694307e-53\n",
      "Score at step  248 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.487358021816584e-54\n",
      "Score at step  249 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.69241481308995e-54\n",
      "Score at step  250 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.41544888785397e-54\n",
      "Score at step  251 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.049269332712382e-54\n",
      "Score at step  252 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.229561599627429e-54\n",
      "Score at step  253 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.377369597764574e-55\n",
      "Score at step  254 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.426421758658744e-55\n",
      "Score at step  255 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.6558530551952465e-55\n",
      "Score at step  256 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5935118331171479e-55\n",
      "Score at step  257 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.561070998702888e-56\n",
      "Score at step  258 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.736642599221733e-56\n",
      "Score at step  259 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.4419855595330397e-56\n",
      "Score at step  260 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.065191335719824e-56\n",
      "Score at step  261 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2391148014318942e-56\n",
      "Score at step  262 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.434688808591365e-57\n",
      "Score at step  263 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.4608132851548185e-57\n",
      "Score at step  264 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.676487971092891e-57\n",
      "Score at step  265 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.6058927826557346e-57\n",
      "Score at step  266 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.635356695934406e-58\n",
      "Score at step  267 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.7812140175606435e-58\n",
      "Score at step  268 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.468728410536386e-58\n",
      "Score at step  269 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.0812370463218313e-58\n",
      "Score at step  270 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2487422277930988e-58\n",
      "Score at step  271 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.492453366758592e-59\n",
      "Score at step  272 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.495472020055155e-59\n",
      "Score at step  273 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.697283212033093e-59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  274 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.6183699272198558e-59\n",
      "Score at step  275 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.710219563319135e-60\n",
      "Score at step  276 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.826131737991481e-60\n",
      "Score at step  277 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.4956790427948885e-60\n",
      "Score at step  278 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.097407425676933e-60\n",
      "Score at step  279 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2584444554061598e-60\n",
      "Score at step  280 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.550666732436959e-61\n",
      "Score at step  281 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.5304000394621755e-61\n",
      "Score at step  282 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.718240023677305e-61\n",
      "Score at step  283 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.630944014206383e-61\n",
      "Score at step  284 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.785664085238298e-62\n",
      "Score at step  285 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.871398451142979e-62\n",
      "Score at step  286 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.522839070685787e-62\n",
      "Score at step  287 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.113703442411472e-62\n",
      "Score at step  288 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2682220654468831e-62\n",
      "Score at step  289 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.609332392681298e-63\n",
      "Score at step  290 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.5655994356087784e-63\n",
      "Score at step  291 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.739359661365267e-63\n",
      "Score at step  292 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.64361579681916e-63\n",
      "Score at step  293 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.86169478091496e-64\n",
      "Score at step  294 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.917016868548976e-64\n",
      "Score at step  295 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.5502101211293856e-64\n",
      "Score at step  296 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.130126072677631e-64\n",
      "Score at step  297 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2780756436065786e-64\n",
      "Score at step  298 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.668453861639471e-65\n",
      "Score at step  299 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.601072316983682e-65\n",
      "Score at step  300 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.760643390190209e-65\n",
      "Score at step  301 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.6563860341141255e-65\n",
      "Score at step  302 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.938316204684752e-66\n",
      "Score at step  303 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.962989722810851e-66\n",
      "Score at step  304 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.5777938336865106e-66\n",
      "Score at step  305 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.1466763002119063e-66\n",
      "Score at step  306 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2880057801271437e-66\n",
      "Score at step  307 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.728034680762862e-67\n",
      "Score at step  308 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.636820808457717e-67\n",
      "Score at step  309 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.7820924850746304e-67\n",
      "Score at step  310 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.6692554910447783e-67\n",
      "Score at step  311 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0015532946268669e-67\n",
      "Score at step  312 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.0093197677612015e-68\n",
      "Score at step  313 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.6055918606567207e-68\n",
      "Score at step  314 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.163355116394032e-68\n",
      "Score at step  315 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.2980130698364192e-68\n",
      "Score at step  316 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.788078419018515e-69\n",
      "Score at step  317 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.672847051411109e-69\n",
      "Score at step  318 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.8037082308466653e-69\n",
      "Score at step  319 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.6822249385079992e-69\n",
      "Score at step  320 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0093349631047995e-69\n",
      "Score at step  321 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.056009778628797e-70\n",
      "Score at step  322 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.633605867177278e-70\n",
      "Score at step  323 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.180163520306367e-70\n",
      "Score at step  324 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3080981121838202e-70\n",
      "Score at step  325 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.848588673102921e-71\n",
      "Score at step  326 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.709153203861752e-71\n",
      "Score at step  327 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.8254919223170513e-71\n",
      "Score at step  328 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.6952951533902307e-71\n",
      "Score at step  329 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0171770920341384e-71\n",
      "Score at step  330 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.10306255220483e-72\n",
      "Score at step  331 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.661837531322898e-72\n",
      "Score at step  332 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.1971025187937385e-72\n",
      "Score at step  333 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.318261511276243e-72\n",
      "Score at step  334 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.909569067657458e-73\n",
      "Score at step  335 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.7457414405944743e-73\n",
      "Score at step  336 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.8474448643566843e-73\n",
      "Score at step  337 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7084669186140106e-73\n",
      "Score at step  338 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0250801511684063e-73\n",
      "Score at step  339 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.150480907010438e-74\n",
      "Score at step  340 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.690288544206262e-74\n",
      "Score at step  341 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.214173126523757e-74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  342 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3285038759142543e-74\n",
      "Score at step  343 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.971023255485526e-75\n",
      "Score at step  344 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.782613953291315e-75\n",
      "Score at step  345 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.869568371974789e-75\n",
      "Score at step  346 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7217410231848733e-75\n",
      "Score at step  347 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.033044613910924e-75\n",
      "Score at step  348 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.198267683465544e-76\n",
      "Score at step  349 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.718960610079326e-76\n",
      "Score at step  350 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2313763660475958e-76\n",
      "Score at step  351 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3388258196285573e-76\n",
      "Score at step  352 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.032954917771343e-77\n",
      "Score at step  353 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.8197729506628054e-77\n",
      "Score at step  354 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.891863770397683e-77\n",
      "Score at step  355 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.73511826223861e-77\n",
      "Score at step  356 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0410709573431659e-77\n",
      "Score at step  357 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.246425744058995e-78\n",
      "Score at step  358 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.747855446435397e-78\n",
      "Score at step  359 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.248713267861238e-78\n",
      "Score at step  360 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3492279607167426e-78\n",
      "Score at step  361 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.095367764300455e-79\n",
      "Score at step  362 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.857220658580273e-79\n",
      "Score at step  363 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.914332395148164e-79\n",
      "Score at step  364 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7485994370888983e-79\n",
      "Score at step  365 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0491596622533389e-79\n",
      "Score at step  366 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.294957973520033e-80\n",
      "Score at step  367 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.776974784112019e-80\n",
      "Score at step  368 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2661848704672114e-80\n",
      "Score at step  369 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3597109222803267e-80\n",
      "Score at step  370 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.15826553368196e-81\n",
      "Score at step  371 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.894959320209176e-81\n",
      "Score at step  372 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.9369755921255054e-81\n",
      "Score at step  373 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.762185355275303e-81\n",
      "Score at step  374 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0573112131651817e-81\n",
      "Score at step  375 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.34386727899109e-82\n",
      "Score at step  376 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.806320367394654e-82\n",
      "Score at step  377 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.2837922204367922e-82\n",
      "Score at step  378 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3702753322620754e-82\n",
      "Score at step  379 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.221651993572452e-83\n",
      "Score at step  380 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.932991196143471e-83\n",
      "Score at step  381 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.9597947176860826e-83\n",
      "Score at step  382 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.7758768306116497e-83\n",
      "Score at step  383 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0655260983669898e-83\n",
      "Score at step  384 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.393156590201938e-84\n",
      "Score at step  385 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.8358939541211624e-84\n",
      "Score at step  386 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.301536372472697e-84\n",
      "Score at step  387 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3809218234836183e-84\n",
      "Score at step  388 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.28553094090171e-85\n",
      "Score at step  389 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.9713185645410253e-85\n",
      "Score at step  390 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.982791138724615e-85\n",
      "Score at step  391 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.789674683234769e-85\n",
      "Score at step  392 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0738048099408614e-85\n",
      "Score at step  393 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.442828859645168e-86\n",
      "Score at step  394 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.865697315787101e-86\n",
      "Score at step  395 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.3194183894722603e-86\n",
      "Score at step  396 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.3916510336833561e-86\n",
      "Score at step  397 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.349906202100137e-87\n",
      "Score at step  398 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.009943721260082e-87\n",
      "Score at step  399 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.0059662327560493e-87\n",
      "Score at step  400 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8035797396536294e-87\n",
      "Score at step  401 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.0821478437921776e-87\n",
      "Score at step  402 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.492887062753065e-88\n",
      "Score at step  403 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.895732237651839e-88\n",
      "Score at step  404 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.3374393425911033e-88\n",
      "Score at step  405 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.402463605554662e-88\n",
      "Score at step  406 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.414781633327971e-89\n",
      "Score at step  407 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.048868979996782e-89\n",
      "Score at step  408 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.0293213879980693e-89\n",
      "Score at step  409 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8175928327988417e-89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  410 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.090555699679305e-89\n",
      "Score at step  411 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.54333419807583e-90\n",
      "Score at step  412 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.926000518845498e-90\n",
      "Score at step  413 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.3556003113072986e-90\n",
      "Score at step  414 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4133601867843791e-90\n",
      "Score at step  415 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.480161120706275e-91\n",
      "Score at step  416 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.088096672423765e-91\n",
      "Score at step  417 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.0528580034542586e-91\n",
      "Score at step  418 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8317148020725552e-91\n",
      "Score at step  419 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.099028881243533e-91\n",
      "Score at step  420 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.594173287461197e-92\n",
      "Score at step  421 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.956503972476718e-92\n",
      "Score at step  422 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.373902383486031e-92\n",
      "Score at step  423 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4243414300916184e-92\n",
      "Score at step  424 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.54604858054971e-93\n",
      "Score at step  425 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.127629148329826e-93\n",
      "Score at step  426 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.0765774889978955e-93\n",
      "Score at step  427 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8459464933987372e-93\n",
      "Score at step  428 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1075678960392423e-93\n",
      "Score at step  429 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.645407376235454e-94\n",
      "Score at step  430 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.987244425741272e-94\n",
      "Score at step  431 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.392346655444763e-94\n",
      "Score at step  432 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4354079932668578e-94\n",
      "Score at step  433 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.612447959601146e-95\n",
      "Score at step  434 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.167468775760688e-95\n",
      "Score at step  435 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.1004812654564125e-95\n",
      "Score at step  436 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8602887592738474e-95\n",
      "Score at step  437 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1161732555643083e-95\n",
      "Score at step  438 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.69703953338585e-96\n",
      "Score at step  439 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.01822372003151e-96\n",
      "Score at step  440 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.410934232018906e-96\n",
      "Score at step  441 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4465605392113436e-96\n",
      "Score at step  442 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.679363235268061e-97\n",
      "Score at step  443 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.207617941160836e-97\n",
      "Score at step  444 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.1245707646965015e-97\n",
      "Score at step  445 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.874742458817901e-97\n",
      "Score at step  446 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1248454752907405e-97\n",
      "Score at step  447 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.749072851744443e-98\n",
      "Score at step  448 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.049443711046666e-98\n",
      "Score at step  449 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.4296662266279993e-98\n",
      "Score at step  450 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4577997359767995e-98\n",
      "Score at step  451 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.746798415860796e-99\n",
      "Score at step  452 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.248079049516477e-99\n",
      "Score at step  453 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.1488474297098865e-99\n",
      "Score at step  454 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.8893084578259316e-99\n",
      "Score at step  455 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.133585074695559e-99\n",
      "Score at step  456 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.801510448173354e-100\n",
      "Score at step  457 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.080906268904012e-100\n",
      "Score at step  458 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.4485437613424074e-100\n",
      "Score at step  459 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4691262568054445e-100\n",
      "Score at step  460 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.814757540832667e-101\n",
      "Score at step  461 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.2888545244995996e-101\n",
      "Score at step  462 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.17331271469976e-101\n",
      "Score at step  463 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9039876288198557e-101\n",
      "Score at step  464 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1423925772919134e-101\n",
      "Score at step  465 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.8543554637514805e-102\n",
      "Score at step  466 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.1126132782508884e-102\n",
      "Score at step  467 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.467567966950533e-102\n",
      "Score at step  468 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4805407801703196e-102\n",
      "Score at step  469 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.883244681021917e-103\n",
      "Score at step  470 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.32994680861315e-103\n",
      "Score at step  471 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.19796808516789e-103\n",
      "Score at step  472 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.918780851100734e-103\n",
      "Score at step  473 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1512685106604403e-103\n",
      "Score at step  474 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.907611063962642e-104\n",
      "Score at step  475 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.144566638377585e-104\n",
      "Score at step  476 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.4867399830265508e-104\n",
      "Score at step  477 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.4920439898159305e-104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  478 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  8.952263938895583e-105\n",
      "Score at step  479 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.37135836333735e-105\n",
      "Score at step  480 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.2228150180024095e-105\n",
      "Score at step  481 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.9336890108014457e-105\n",
      "Score at step  482 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1602134064808673e-105\n",
      "Score at step  483 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  6.961280438885203e-106\n",
      "Score at step  484 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.1767682633311215e-106\n",
      "Score at step  485 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.5060609579986726e-106\n",
      "Score at step  486 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5036365747992035e-106\n",
      "Score at step  487 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.021819448795221e-107\n",
      "Score at step  488 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.413091669277132e-107\n",
      "Score at step  489 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.247855001566279e-107\n",
      "Score at step  490 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.948713000939767e-107\n",
      "Score at step  491 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.1692278005638602e-107\n",
      "Score at step  492 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  7.015366803383161e-108\n",
      "Score at step  493 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  4.209220082029897e-108\n",
      "Score at step  494 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  2.525532049217938e-108\n",
      "Score at step  495 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.5153192295307626e-108\n",
      "Score at step  496 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  9.091915377184575e-109\n",
      "Score at step  497 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  5.455149226310745e-109\n",
      "Score at step  498 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  3.273089535786447e-109\n",
      "Score at step  499 = tensor([0.4511], grad_fn=<NegBackward>) max grad component = tensor(-0.) lr =  1.963853721471868e-109\n",
      "Max(Y) =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "['▁Ich', '▁denke', ',', '▁dass', '▁die', '▁', 'maschine', 'lle', '▁Übersetzung', '▁ein', '▁sehr', '▁interessante', 's', '▁Thema', '▁ist', '.']\n"
     ]
    }
   ],
   "source": [
    "english_sentence = 'I think, that machine translation is a very interesting subject.'\n",
    "english_tok = tokenizer.tokenize(english_sentence)\n",
    "greedy_optimizer = GreedyOptimizer(english_sentence)\n",
    "greedy_translation = greedy_optimizer.optimize()[:-1]\n",
    "greedy_init = one_hot_encoder.encode(torch.tensor([[[vocab.stoi[tok] for tok in greedy_translation]]]).transpose(0, 2), v=1000.).squeeze().transpose(0, 1)\n",
    "\n",
    "optimizer = ContinuousOptimizer(english_sentence)\n",
    "print('Optimization starts...')\n",
    "res = optimizer.optimize(init=greedy_init)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy translation: ['▁Ich', '▁denke', ',', '▁dass', '▁die', '▁', 'maschine', 'lle', '▁Übersetzung', '▁ein', '▁sehr', '▁interessante', 's', '▁Thema', '▁ist', '.']\n",
      "Continuous translation: ['▁Ich', '▁denke', ',', '▁dass', '▁die', '▁', 'maschine', 'lle', '▁Übersetzung', '▁ein', '▁sehr', '▁interessante', 's', '▁Thema', '▁ist', '.']\n",
      "Score of greedy translation =  tensor([-0.4511])\n",
      "Score of continuous translation =  tensor([-0.4511])\n"
     ]
    }
   ],
   "source": [
    "print('Greedy translation:', greedy_translation)\n",
    "print('Continuous translation:', res)\n",
    "print('Score of greedy translation = ' , scorer.score_tokenized_texts([english_tok], [greedy_translation]))\n",
    "print('Score of continuous translation = ', scorer.score_tokenized_texts([english_tok], [res]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy initialization with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization starts...\n",
      "Score at step  0 = tensor([1.8099], grad_fn=<NegBackward>) max grad component =  lr =  99.0\n",
      "Score at step  1 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.) lr =  98.01\n",
      "Score at step  2 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  58.806\n",
      "Score at step  3 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  35.2836\n",
      "Score at step  4 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  21.17016\n",
      "Score at step  5 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  12.702096\n",
      "Score at step  6 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.621257599999999\n",
      "Score at step  7 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.572754559999999\n",
      "Score at step  8 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.743652735999999\n",
      "Score at step  9 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6461916415999995\n",
      "Score at step  10 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.9877149849599997\n",
      "Score at step  11 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.5926289909759997\n",
      "Score at step  12 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.35557739458559984\n",
      "Score at step  13 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.2133464367513599\n",
      "Score at step  14 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.12800786205081593\n",
      "Score at step  15 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.07680471723048955\n",
      "Score at step  16 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.04608283033829373\n",
      "Score at step  17 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.027649698202976237\n",
      "Score at step  18 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.01658981892178574\n",
      "Score at step  19 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.009953891353071445\n",
      "Score at step  20 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.0059723348118428665\n",
      "Score at step  21 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.0035834008871057197\n",
      "Score at step  22 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.002150040532263432\n",
      "Score at step  23 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.0012900243193580592\n",
      "Score at step  24 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.0007740145916148355\n",
      "Score at step  25 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.0004644087549689013\n",
      "Score at step  26 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.00027864525298134074\n",
      "Score at step  27 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.00016718715178880444\n",
      "Score at step  28 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  0.00010031229107328266\n",
      "Score at step  29 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.018737464396959e-05\n",
      "Score at step  30 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.611242478638175e-05\n",
      "Score at step  31 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.166745487182905e-05\n",
      "Score at step  32 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.300047292309743e-05\n",
      "Score at step  33 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.800283753858458e-06\n",
      "Score at step  34 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.680170252315075e-06\n",
      "Score at step  35 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.808102151389045e-06\n",
      "Score at step  36 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.684861290833427e-06\n",
      "Score at step  37 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.010916774500056e-06\n",
      "Score at step  38 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.065500647000336e-07\n",
      "Score at step  39 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.6393003882002015e-07\n",
      "Score at step  40 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.1835802329201207e-07\n",
      "Score at step  41 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3101481397520724e-07\n",
      "Score at step  42 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.860888838512434e-08\n",
      "Score at step  43 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.7165333031074604e-08\n",
      "Score at step  44 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.8299199818644762e-08\n",
      "Score at step  45 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6979519891186855e-08\n",
      "Score at step  46 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0187711934712112e-08\n",
      "Score at step  47 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.1126271608272674e-09\n",
      "Score at step  48 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.6675762964963602e-09\n",
      "Score at step  49 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.200545777897816e-09\n",
      "Score at step  50 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3203274667386895e-09\n",
      "Score at step  51 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.921964800432137e-10\n",
      "Score at step  52 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.753178880259282e-10\n",
      "Score at step  53 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.8519073281555687e-10\n",
      "Score at step  54 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7111443968933412e-10\n",
      "Score at step  55 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0266866381360047e-10\n",
      "Score at step  56 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.160119828816028e-11\n",
      "Score at step  57 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.696071897289616e-11\n",
      "Score at step  58 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.2176431383737696e-11\n",
      "Score at step  59 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3305858830242618e-11\n",
      "Score at step  60 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.98351529814557e-12\n",
      "Score at step  61 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.790109178887342e-12\n",
      "Score at step  62 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.874065507332405e-12\n",
      "Score at step  63 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.724439304399443e-12\n",
      "Score at step  64 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0346635826396657e-12\n",
      "Score at step  65 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.207981495837994e-13\n",
      "Score at step  66 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.7247888975027964e-13\n",
      "Score at step  67 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.2348733385016778e-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  68 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3409240031010065e-13\n",
      "Score at step  69 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.045544018606039e-14\n",
      "Score at step  70 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.827326411163623e-14\n",
      "Score at step  71 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.8963958466981735e-14\n",
      "Score at step  72 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.737837508018904e-14\n",
      "Score at step  73 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0427025048113424e-14\n",
      "Score at step  74 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.256215028868054e-15\n",
      "Score at step  75 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.7537290173208325e-15\n",
      "Score at step  76 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.2522374103924995e-15\n",
      "Score at step  77 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3513424462354997e-15\n",
      "Score at step  78 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.108054677412998e-16\n",
      "Score at step  79 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.864832806447799e-16\n",
      "Score at step  80 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.918899683868679e-16\n",
      "Score at step  81 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7513398103212073e-16\n",
      "Score at step  82 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0508038861927244e-16\n",
      "Score at step  83 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.304823317156346e-17\n",
      "Score at step  84 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.7828939902938076e-17\n",
      "Score at step  85 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.2697363941762846e-17\n",
      "Score at step  86 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3618418365057707e-17\n",
      "Score at step  87 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.171051019034624e-18\n",
      "Score at step  88 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.902630611420774e-18\n",
      "Score at step  89 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.9415783668524645e-18\n",
      "Score at step  90 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7649470201114787e-18\n",
      "Score at step  91 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.058968212066887e-18\n",
      "Score at step  92 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.353809272401322e-19\n",
      "Score at step  93 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.8122855634407935e-19\n",
      "Score at step  94 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.287371338064476e-19\n",
      "Score at step  95 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3724228028386858e-19\n",
      "Score at step  96 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.234536817032114e-20\n",
      "Score at step  97 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.9407220902192684e-20\n",
      "Score at step  98 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.964433254131561e-20\n",
      "Score at step  99 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7786599524789367e-20\n",
      "Score at step  100 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.067195971487362e-20\n",
      "Score at step  101 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.403175828924172e-21\n",
      "Score at step  102 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.841905497354503e-21\n",
      "Score at step  103 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.3051432984127015e-21\n",
      "Score at step  104 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3830859790476209e-21\n",
      "Score at step  105 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.2985158742857245e-22\n",
      "Score at step  106 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.979109524571435e-22\n",
      "Score at step  107 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.987465714742861e-22\n",
      "Score at step  108 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7924794288457165e-22\n",
      "Score at step  109 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0754876573074298e-22\n",
      "Score at step  110 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.452925943844578e-23\n",
      "Score at step  111 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.871755566306747e-23\n",
      "Score at step  112 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.323053339784048e-23\n",
      "Score at step  113 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.393832003870429e-23\n",
      "Score at step  114 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.362992023222573e-24\n",
      "Score at step  115 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.0177952139335436e-24\n",
      "Score at step  116 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.010677128360126e-24\n",
      "Score at step  117 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8064062770160755e-24\n",
      "Score at step  118 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0838437662096453e-24\n",
      "Score at step  119 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.503062597257871e-25\n",
      "Score at step  120 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.9018375583547225e-25\n",
      "Score at step  121 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.341102535012833e-25\n",
      "Score at step  122 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4046615210076998e-25\n",
      "Score at step  123 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.427969126046199e-26\n",
      "Score at step  124 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.0567814756277194e-26\n",
      "Score at step  125 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.034068885376632e-26\n",
      "Score at step  126 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.820441331225979e-26\n",
      "Score at step  127 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0922647987355874e-26\n",
      "Score at step  128 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.553588792413524e-27\n",
      "Score at step  129 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.9321532754481144e-27\n",
      "Score at step  130 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.3592919652688686e-27\n",
      "Score at step  131 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4155751791613212e-27\n",
      "Score at step  132 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.493451074967927e-28\n",
      "Score at step  133 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.096070644980756e-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  134 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.0576423869884536e-28\n",
      "Score at step  135 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.834585432193072e-28\n",
      "Score at step  136 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.100751259315843e-28\n",
      "Score at step  137 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.604507555895059e-29\n",
      "Score at step  138 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.962704533537035e-29\n",
      "Score at step  139 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.377622720122221e-29\n",
      "Score at step  140 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4265736320733325e-29\n",
      "Score at step  141 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.559441792439995e-30\n",
      "Score at step  142 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.135665075463997e-30\n",
      "Score at step  143 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.081399045278398e-30\n",
      "Score at step  144 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8488394271670387e-30\n",
      "Score at step  145 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1093036563002232e-30\n",
      "Score at step  146 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.655821937801339e-31\n",
      "Score at step  147 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.993493162680804e-31\n",
      "Score at step  148 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.396095897608482e-31\n",
      "Score at step  149 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4376575385650892e-31\n",
      "Score at step  150 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.625945231390535e-32\n",
      "Score at step  151 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.17556713883432e-32\n",
      "Score at step  152 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.1053402833005923e-32\n",
      "Score at step  153 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8632041699803553e-32\n",
      "Score at step  154 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1179225019882131e-32\n",
      "Score at step  155 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.707535011929279e-33\n",
      "Score at step  156 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.024521007157567e-33\n",
      "Score at step  157 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4147126042945404e-33\n",
      "Score at step  158 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4488275625767241e-33\n",
      "Score at step  159 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.692965375460344e-34\n",
      "Score at step  160 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.215779225276206e-34\n",
      "Score at step  161 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.1294675351657236e-34\n",
      "Score at step  162 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.877680521099434e-34\n",
      "Score at step  163 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1266083126596603e-34\n",
      "Score at step  164 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.759649875957961e-35\n",
      "Score at step  165 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.055789925574777e-35\n",
      "Score at step  166 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.433473955344866e-35\n",
      "Score at step  167 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4600843732069196e-35\n",
      "Score at step  168 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.760506239241517e-36\n",
      "Score at step  169 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.25630374354491e-36\n",
      "Score at step  170 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.153782246126946e-36\n",
      "Score at step  171 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8922693476761675e-36\n",
      "Score at step  172 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1353616086057004e-36\n",
      "Score at step  173 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.812169651634202e-37\n",
      "Score at step  174 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.087301790980521e-37\n",
      "Score at step  175 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4523810745883125e-37\n",
      "Score at step  176 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4714286447529876e-37\n",
      "Score at step  177 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.828571868517925e-38\n",
      "Score at step  178 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.297143121110755e-38\n",
      "Score at step  179 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.178285872666453e-38\n",
      "Score at step  180 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9069715235998718e-38\n",
      "Score at step  181 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.144182914159923e-38\n",
      "Score at step  182 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.865097484959538e-39\n",
      "Score at step  183 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.1190584909757226e-39\n",
      "Score at step  184 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4714350945854334e-39\n",
      "Score at step  185 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.48286105675126e-39\n",
      "Score at step  186 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.897166340507559e-40\n",
      "Score at step  187 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.338299804304535e-40\n",
      "Score at step  188 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.202979882582721e-40\n",
      "Score at step  189 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9217879295496324e-40\n",
      "Score at step  190 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1530727577297794e-40\n",
      "Score at step  191 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.918436546378676e-41\n",
      "Score at step  192 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.151061927827206e-41\n",
      "Score at step  193 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4906371566963234e-41\n",
      "Score at step  194 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.494382294017794e-41\n",
      "Score at step  195 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.966293764106764e-42\n",
      "Score at step  196 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.379776258464058e-42\n",
      "Score at step  197 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.2278657550784345e-42\n",
      "Score at step  198 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9367194530470607e-42\n",
      "Score at step  199 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1620316718282365e-42\n",
      "Score at step  200 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.972190030969418e-43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  201 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.183314018581651e-43\n",
      "Score at step  202 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.5099884111489904e-43\n",
      "Score at step  203 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.5059930466893943e-43\n",
      "Score at step  204 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.035958280136365e-44\n",
      "Score at step  205 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.421574968081819e-44\n",
      "Score at step  206 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.2529449808490915e-44\n",
      "Score at step  207 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9517669885094548e-44\n",
      "Score at step  208 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1710601931056727e-44\n",
      "Score at step  209 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.026361158634036e-45\n",
      "Score at step  210 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.2158166951804213e-45\n",
      "Score at step  211 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.5294900171082527e-45\n",
      "Score at step  212 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.5176940102649515e-45\n",
      "Score at step  213 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.106164061589709e-46\n",
      "Score at step  214 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.463698436953825e-46\n",
      "Score at step  215 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.278219062172295e-46\n",
      "Score at step  216 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.966931437303377e-46\n",
      "Score at step  217 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.180158862382026e-46\n",
      "Score at step  218 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.080953174292156e-47\n",
      "Score at step  219 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.2485719045752935e-47\n",
      "Score at step  220 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.549143142745176e-47\n",
      "Score at step  221 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.5294858856471056e-47\n",
      "Score at step  222 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.176915313882633e-48\n",
      "Score at step  223 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.506149188329579e-48\n",
      "Score at step  224 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.3036895129977473e-48\n",
      "Score at step  225 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9822137077986485e-48\n",
      "Score at step  226 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.189328224679189e-48\n",
      "Score at step  227 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.135969348075133e-49\n",
      "Score at step  228 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.28158160884508e-49\n",
      "Score at step  229 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.5689489653070477e-49\n",
      "Score at step  230 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.5413693791842286e-49\n",
      "Score at step  231 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.248216275105372e-50\n",
      "Score at step  232 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.548929765063223e-50\n",
      "Score at step  233 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.3293578590379334e-50\n",
      "Score at step  234 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9976147154227599e-50\n",
      "Score at step  235 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1985688292536559e-50\n",
      "Score at step  236 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.191412975521935e-51\n",
      "Score at step  237 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.314847785313161e-51\n",
      "Score at step  238 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.5889086711878964e-51\n",
      "Score at step  239 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.553345202712738e-51\n",
      "Score at step  240 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.320071216276427e-52\n",
      "Score at step  241 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.592042729765856e-52\n",
      "Score at step  242 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.355225637859514e-52\n",
      "Score at step  243 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.0131353827157084e-52\n",
      "Score at step  244 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.207881229629425e-52\n",
      "Score at step  245 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.24728737777655e-53\n",
      "Score at step  246 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.34837242666593e-53\n",
      "Score at step  247 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.609023455999558e-53\n",
      "Score at step  248 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.5654140735997347e-53\n",
      "Score at step  249 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.392484441598408e-54\n",
      "Score at step  250 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.635490664959045e-54\n",
      "Score at step  251 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.381294398975427e-54\n",
      "Score at step  252 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.028776639385256e-54\n",
      "Score at step  253 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2172659836311535e-54\n",
      "Score at step  254 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.30359590178692e-55\n",
      "Score at step  255 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.382157541072152e-55\n",
      "Score at step  256 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.6292945246432913e-55\n",
      "Score at step  257 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.5775767147859747e-55\n",
      "Score at step  258 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.465460288715848e-56\n",
      "Score at step  259 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.679276173229509e-56\n",
      "Score at step  260 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.407565703937705e-56\n",
      "Score at step  261 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.044539422362623e-56\n",
      "Score at step  262 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2267236534175736e-56\n",
      "Score at step  263 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.360341920505442e-57\n",
      "Score at step  264 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.416205152303265e-57\n",
      "Score at step  265 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.6497230913819585e-57\n",
      "Score at step  266 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.589833854829175e-57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  267 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.539003128975051e-58\n",
      "Score at step  268 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.72340187738503e-58\n",
      "Score at step  269 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.434041126431018e-58\n",
      "Score at step  270 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.060424675858611e-58\n",
      "Score at step  271 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2362548055151665e-58\n",
      "Score at step  272 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.417528833090999e-59\n",
      "Score at step  273 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.450517299854599e-59\n",
      "Score at step  274 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.670310379912759e-59\n",
      "Score at step  275 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6021862279476554e-59\n",
      "Score at step  276 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.613117367685932e-60\n",
      "Score at step  277 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.767870420611559e-60\n",
      "Score at step  278 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.4607222523669354e-60\n",
      "Score at step  279 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.0764333514201612e-60\n",
      "Score at step  280 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2458600108520966e-60\n",
      "Score at step  281 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.475160065112579e-61\n",
      "Score at step  282 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.485096039067547e-61\n",
      "Score at step  283 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.691057623440528e-61\n",
      "Score at step  284 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.614634574064317e-61\n",
      "Score at step  285 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.687807444385901e-62\n",
      "Score at step  286 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.81268446663154e-62\n",
      "Score at step  287 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.487610679978924e-62\n",
      "Score at step  288 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.0925664079873544e-62\n",
      "Score at step  289 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2555398447924127e-62\n",
      "Score at step  290 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.533239068754476e-63\n",
      "Score at step  291 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.5199434412526854e-63\n",
      "Score at step  292 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.7119660647516113e-63\n",
      "Score at step  293 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6271796388509669e-63\n",
      "Score at step  294 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.7630778331058e-64\n",
      "Score at step  295 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.85784669986348e-64\n",
      "Score at step  296 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.5147080199180875e-64\n",
      "Score at step  297 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.1088248119508524e-64\n",
      "Score at step  298 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2652948871705115e-64\n",
      "Score at step  299 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.591769323023069e-65\n",
      "Score at step  300 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.555061593813841e-65\n",
      "Score at step  301 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.7330369562883046e-65\n",
      "Score at step  302 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6398221737729828e-65\n",
      "Score at step  303 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.838933042637896e-66\n",
      "Score at step  304 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.903359825582737e-66\n",
      "Score at step  305 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.542015895349642e-66\n",
      "Score at step  306 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.1252095372097853e-66\n",
      "Score at step  307 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2751257223258712e-66\n",
      "Score at step  308 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.650754333955227e-67\n",
      "Score at step  309 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.590452600373136e-67\n",
      "Score at step  310 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.7542715602238814e-67\n",
      "Score at step  311 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6525629361343288e-67\n",
      "Score at step  312 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.915377616805972e-68\n",
      "Score at step  313 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.949226570083583e-68\n",
      "Score at step  314 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.5695359420501494e-68\n",
      "Score at step  315 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.1417215652300896e-68\n",
      "Score at step  316 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2850329391380537e-68\n",
      "Score at step  317 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.710197634828323e-69\n",
      "Score at step  318 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.6261185808969934e-69\n",
      "Score at step  319 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.775671148538196e-69\n",
      "Score at step  320 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6654026891229176e-69\n",
      "Score at step  321 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.992416134737505e-70\n",
      "Score at step  322 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.995449680842502e-70\n",
      "Score at step  323 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.597269808505501e-70\n",
      "Score at step  324 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.1583618851033008e-70\n",
      "Score at step  325 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.2950171310619805e-70\n",
      "Score at step  326 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.770102786371882e-71\n",
      "Score at step  327 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.662061671823129e-71\n",
      "Score at step  328 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.7972370030938772e-71\n",
      "Score at step  329 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6783422018563263e-71\n",
      "Score at step  330 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0070053211137956e-71\n",
      "Score at step  331 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.042031926682773e-72\n",
      "Score at step  332 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.625219156009664e-72\n",
      "Score at step  333 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.1751314936057983e-72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  334 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.305078896163479e-72\n",
      "Score at step  335 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.830473376980874e-73\n",
      "Score at step  336 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.698284026188524e-73\n",
      "Score at step  337 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.8189704157131146e-73\n",
      "Score at step  338 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.6913822494278687e-73\n",
      "Score at step  339 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0148293496567212e-73\n",
      "Score at step  340 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.088976097940327e-74\n",
      "Score at step  341 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.653385658764196e-74\n",
      "Score at step  342 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.192031395258518e-74\n",
      "Score at step  343 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3152188371551107e-74\n",
      "Score at step  344 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.891313022930664e-75\n",
      "Score at step  345 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.734787813758398e-75\n",
      "Score at step  346 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.840872688255039e-75\n",
      "Score at step  347 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7045236129530233e-75\n",
      "Score at step  348 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.022714167771814e-75\n",
      "Score at step  349 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.136285006630883e-76\n",
      "Score at step  350 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.68177100397853e-76\n",
      "Score at step  351 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.209062602387118e-76\n",
      "Score at step  352 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3254375614322709e-76\n",
      "Score at step  353 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  7.952625368593624e-77\n",
      "Score at step  354 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.7715752211561745e-77\n",
      "Score at step  355 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.8629451326937045e-77\n",
      "Score at step  356 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7177670796162227e-77\n",
      "Score at step  357 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0306602477697335e-77\n",
      "Score at step  358 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.183961486618401e-78\n",
      "Score at step  359 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.7103768919710405e-78\n",
      "Score at step  360 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.2262261351826244e-78\n",
      "Score at step  361 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3357356811095746e-78\n",
      "Score at step  362 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.014414086657448e-79\n",
      "Score at step  363 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.8086484519944686e-79\n",
      "Score at step  364 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.885189071196681e-79\n",
      "Score at step  365 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7311134427180086e-79\n",
      "Score at step  366 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0386680656308051e-79\n",
      "Score at step  367 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.23200839378483e-80\n",
      "Score at step  368 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.739205036270898e-80\n",
      "Score at step  369 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.243523021762539e-80\n",
      "Score at step  370 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3461138130575233e-80\n",
      "Score at step  371 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.076682878345139e-81\n",
      "Score at step  372 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.846009727007083e-81\n",
      "Score at step  373 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.90760583620425e-81\n",
      "Score at step  374 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.74456350172255e-81\n",
      "Score at step  375 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.04673810103353e-81\n",
      "Score at step  376 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.28042860620118e-82\n",
      "Score at step  377 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.7682571637207077e-82\n",
      "Score at step  378 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.2609542982324247e-82\n",
      "Score at step  379 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3565725789394547e-82\n",
      "Score at step  380 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.139435473636727e-83\n",
      "Score at step  381 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.883661284182036e-83\n",
      "Score at step  382 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.9301967705092213e-83\n",
      "Score at step  383 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7581180623055326e-83\n",
      "Score at step  384 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0548708373833196e-83\n",
      "Score at step  385 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.329225024299917e-84\n",
      "Score at step  386 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.7975350145799503e-84\n",
      "Score at step  387 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.27852100874797e-84\n",
      "Score at step  388 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.367112605248782e-84\n",
      "Score at step  389 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.202675631492692e-85\n",
      "Score at step  390 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.921605378895615e-85\n",
      "Score at step  391 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.952963227337369e-85\n",
      "Score at step  392 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7717779364024215e-85\n",
      "Score at step  393 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0630667618414528e-85\n",
      "Score at step  394 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.378400571048717e-86\n",
      "Score at step  395 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.82704034262923e-86\n",
      "Score at step  396 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.296224205577538e-86\n",
      "Score at step  397 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3777345233465228e-86\n",
      "Score at step  398 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.266407140079136e-87\n",
      "Score at step  399 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.959844284047481e-87\n",
      "Score at step  400 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.9759065704284887e-87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  401 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7855439422570933e-87\n",
      "Score at step  402 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.071326365354256e-87\n",
      "Score at step  403 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.427958192125536e-88\n",
      "Score at step  404 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.8567749152753214e-88\n",
      "Score at step  405 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.3140649491651927e-88\n",
      "Score at step  406 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3884389694991155e-88\n",
      "Score at step  407 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.330633816994693e-89\n",
      "Score at step  408 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.998380290196815e-89\n",
      "Score at step  409 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.9990281741180892e-89\n",
      "Score at step  410 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.7994169044708533e-89\n",
      "Score at step  411 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.079650142682512e-89\n",
      "Score at step  412 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.4779008560950715e-90\n",
      "Score at step  413 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.8867405136570427e-90\n",
      "Score at step  414 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.3320443081942255e-90\n",
      "Score at step  415 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.3992265849165353e-90\n",
      "Score at step  416 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.395359509499212e-91\n",
      "Score at step  417 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.037215705699526e-91\n",
      "Score at step  418 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.0223294234197157e-91\n",
      "Score at step  419 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8133976540518294e-91\n",
      "Score at step  420 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.0880385924310976e-91\n",
      "Score at step  421 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.528231554586586e-92\n",
      "Score at step  422 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.916938932751951e-92\n",
      "Score at step  423 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.3501633596511707e-92\n",
      "Score at step  424 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4100980157907024e-92\n",
      "Score at step  425 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.460588094744214e-93\n",
      "Score at step  426 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.076352856846528e-93\n",
      "Score at step  427 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.0458117141079168e-93\n",
      "Score at step  428 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.82748702846475e-93\n",
      "Score at step  429 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.09649221707885e-93\n",
      "Score at step  430 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.578953302473099e-94\n",
      "Score at step  431 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.9473719814838596e-94\n",
      "Score at step  432 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.3684231888903156e-94\n",
      "Score at step  433 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4210539133341893e-94\n",
      "Score at step  434 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.526323480005136e-95\n",
      "Score at step  435 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.1157940880030817e-95\n",
      "Score at step  436 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.069476452801849e-95\n",
      "Score at step  437 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8416858716811095e-95\n",
      "Score at step  438 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1050115230086657e-95\n",
      "Score at step  439 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.630069138051994e-96\n",
      "Score at step  440 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.978041482831197e-96\n",
      "Score at step  441 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.386824889698718e-96\n",
      "Score at step  442 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4320949338192306e-96\n",
      "Score at step  443 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.592569602915383e-97\n",
      "Score at step  444 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.15554176174923e-97\n",
      "Score at step  445 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.0933250570495377e-97\n",
      "Score at step  446 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8559950342297224e-97\n",
      "Score at step  447 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1135970205378334e-97\n",
      "Score at step  448 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.681582123227e-98\n",
      "Score at step  449 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.0089492739362e-98\n",
      "Score at step  450 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4053695643617197e-98\n",
      "Score at step  451 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4432217386170318e-98\n",
      "Score at step  452 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.65933043170219e-99\n",
      "Score at step  453 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.195598259021314e-99\n",
      "Score at step  454 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.117358955412788e-99\n",
      "Score at step  455 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8704153732476726e-99\n",
      "Score at step  456 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1222492239486034e-99\n",
      "Score at step  457 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.73349534369162e-100\n",
      "Score at step  458 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.040097206214972e-100\n",
      "Score at step  459 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4240583237289834e-100\n",
      "Score at step  460 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.45443499423739e-100\n",
      "Score at step  461 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.72660996542434e-101\n",
      "Score at step  462 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.2359659792546035e-101\n",
      "Score at step  463 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.141579587552762e-101\n",
      "Score at step  464 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8849477525316574e-101\n",
      "Score at step  465 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1309686515189944e-101\n",
      "Score at step  466 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.785811909113966e-102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at step  467 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.0714871454683796e-102\n",
      "Score at step  468 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4428922872810275e-102\n",
      "Score at step  469 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4657353723686165e-102\n",
      "Score at step  470 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.794412234211698e-103\n",
      "Score at step  471 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.276647340527019e-103\n",
      "Score at step  472 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.1659884043162114e-103\n",
      "Score at step  473 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.8995930425897267e-103\n",
      "Score at step  474 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1397558255538359e-103\n",
      "Score at step  475 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.838534953323015e-104\n",
      "Score at step  476 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.1031209719938084e-104\n",
      "Score at step  477 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.461872583196285e-104\n",
      "Score at step  478 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.477123549917771e-104\n",
      "Score at step  479 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.862741299506625e-105\n",
      "Score at step  480 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.3176447797039745e-105\n",
      "Score at step  481 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.1905868678223845e-105\n",
      "Score at step  482 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9143521206934306e-105\n",
      "Score at step  483 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1486112724160583e-105\n",
      "Score at step  484 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.89166763449635e-106\n",
      "Score at step  485 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.1350005806978096e-106\n",
      "Score at step  486 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.4810003484186856e-106\n",
      "Score at step  487 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.4886002090512112e-106\n",
      "Score at step  488 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  8.931601254307268e-107\n",
      "Score at step  489 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.35896075258436e-107\n",
      "Score at step  490 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.215376451550616e-107\n",
      "Score at step  491 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.9292258709303696e-107\n",
      "Score at step  492 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.1575355225582217e-107\n",
      "Score at step  493 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  6.94521313534933e-108\n",
      "Score at step  494 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  4.1671278812095976e-108\n",
      "Score at step  495 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  2.5002767287257583e-108\n",
      "Score at step  496 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  1.500166037235455e-108\n",
      "Score at step  497 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  9.00099622341273e-109\n",
      "Score at step  498 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  5.400597734047638e-109\n",
      "Score at step  499 = tensor([1.1511], grad_fn=<NegBackward>) max grad component = tensor(0.0047) lr =  3.2403586404285824e-109\n",
      "Max(Y) =  tensor([0.5400, 0.4233, 0.8910, 0.8083, 0.9998, 1.0000, 0.9428, 0.8772, 0.9618,\n",
      "        0.8294, 0.7878, 0.9456, 0.8443, 0.9186, 0.8570],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "['▁Ich', '▁halte', ',', '▁dass', '▁die', '▁', 'maschine', 'lle', '▁Übersetzung', '▁ein', '▁sehr', '▁interessante', 's', '▁Thema', '▁ist']\n"
     ]
    }
   ],
   "source": [
    "english_sentence = 'I think, that machine translation is a very interesting subject.'\n",
    "english_tok = tokenizer.tokenize(english_sentence)\n",
    "greedy_optimizer = GreedyOptimizer(english_sentence)\n",
    "greedy_translation = greedy_optimizer.optimize()[:-1]\n",
    "greedy_init = scorer.score_probabilities_for_each_word(english_tok, greedy_translation)\n",
    "greedy_init = torch.tensor(greedy_init[:15, :].T)\n",
    "\n",
    "optimizer = ContinuousOptimizer(english_sentence)\n",
    "print('Optimization starts...')\n",
    "res = optimizer.optimize(init=greedy_init)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy translation: ['▁Ich', '▁denke', ',', '▁dass', '▁die', '▁', 'maschine', 'lle', '▁Übersetzung', '▁ein', '▁sehr', '▁interessante', 's', '▁Thema', '▁ist', '.']\n",
      "Continuous translation: ['▁Ich', '▁halte', ',', '▁dass', '▁die', '▁', 'maschine', 'lle', '▁Übersetzung', '▁ein', '▁sehr', '▁interessante', 's', '▁Thema', '▁ist']\n",
      "Score of greedy translation =  tensor([-0.4511])\n",
      "Score of continuous translation =  tensor([-1.6082])\n"
     ]
    }
   ],
   "source": [
    "print('Greedy translation:', greedy_translation)\n",
    "print('Continuous translation:', res)\n",
    "print('Score of greedy translation = ' , scorer.score_tokenized_texts([english_tok], [greedy_translation]))\n",
    "print('Score of continuous translation = ', scorer.score_tokenized_texts([english_tok], [res]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization starts...\n",
      "Step 0 , loss score =  tensor([2.3881], grad_fn=<NegBackward>) max grad component =  lr =  100\n",
      "\t cscore =  tensor([2.3881], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5355])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁würde , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 1 , loss score =  tensor([1.9038], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.9038], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 2 , loss score =  tensor([1.6958], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.6958], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5412])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁will , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 3 , loss score =  tensor([1.6640], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.6640], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 4 , loss score =  tensor([1.6425], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.6425], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 5 , loss score =  tensor([1.6256], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.6256], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 6 , loss score =  tensor([1.6119], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.6119], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 7 , loss score =  tensor([1.6008], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.6008], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 8 , loss score =  tensor([1.5926], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.5926], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 9 , loss score =  tensor([1.5860], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.5860], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 10 , loss score =  tensor([1.5825], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.5825], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 11 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0 lr =  100\n",
      "\t cscore =  tensor([1.5815], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 12 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  50.0\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 13 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  25.0\n",
      "\t cscore =  tensor([1.5826], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 14 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  12.5\n",
      "\t cscore =  tensor([1.5832], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 15 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  6.25\n",
      "\t cscore =  tensor([1.5835], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 16 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  3.125\n",
      "\t cscore =  tensor([1.5837], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 17 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.5625\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 18 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.78125\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 19 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.390625\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 20 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.1953125\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 21 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.09765625\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 22 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.048828125\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 23 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.0244140625\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.01220703125\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 25 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.006103515625\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 26 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.0030517578125\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 27 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.00152587890625\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 28 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.000762939453125\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 29 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.0003814697265625\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 30 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  0.00019073486328125\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 31 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  9.5367431640625e-05\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 32 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  4.76837158203125e-05\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 33 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  2.384185791015625e-05\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 34 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.1920928955078125e-05\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 35 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  5.9604644775390625e-06\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 36 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  2.9802322387695312e-06\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 37 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.4901161193847656e-06\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 38 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  7.450580596923828e-07\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 39 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  3.725290298461914e-07\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 40 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.862645149230957e-07\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 41 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  9.313225746154785e-08\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 42 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  4.6566128730773926e-08\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 43 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  2.3283064365386963e-08\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 44 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.1641532182693481e-08\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 45 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  5.820766091346741e-09\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 46 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  2.9103830456733704e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 47 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.4551915228366852e-09\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 48 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  7.275957614183426e-10\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 49 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  3.637978807091713e-10\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 50 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.8189894035458565e-10\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 51 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  9.094947017729282e-11\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 52 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  4.547473508864641e-11\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 53 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  2.2737367544323206e-11\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 54 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.1368683772161603e-11\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 55 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  5.6843418860808015e-12\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 56 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  2.8421709430404007e-12\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 57 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  1.4210854715202004e-12\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 58 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  7.105427357601002e-13\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Step 59 , loss score =  tensor([1.5815], grad_fn=<NegBackward>) max grad component = 0.0013183180708438158 lr =  3.552713678800501e-13\n",
      "\t cscore =  tensor([1.5838], grad_fn=<NegBackward>)\n",
      "\tdscore =  tensor([0.5352])\n",
      "▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "\n",
      "Max(Y) =  tensor([0.6310, 0.8127, 0.9195, 0.8843, 0.9510, 0.8865, 0.4117, 0.8255, 0.8874,\n",
      "        0.9605, 0.9999, 0.9911, 0.5727, 0.9102, 0.9312, 0.9497, 0.6517, 0.9234,\n",
      "        0.2249, 0.8805, 0.9998, 0.4125, 0.7413, 0.7488, 0.8090, 0.7030, 0.8086,\n",
      "        0.2209, 0.9684, 0.8968, 0.5532], grad_fn=<MaxBackward0>)\n",
      "Final dscore tensor([18.7324])\n",
      "['▁Aber', '▁der', '▁Bruder', '▁des', '▁Opfer', 's', '▁sagt', ',', '▁er', '▁könne', '▁niemand', 'en', '▁denken', ',', '▁der', '▁ihn', '▁verletz', 'en', '▁möchte', ',', '▁und', '▁sagt', ':', '▁\"', 'Es', '▁ging', '▁ihm', '▁endlich', '▁gut', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "english_sentence = 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"'\n",
    "english_tok = tokenizer.tokenize(english_sentence)\n",
    "greedy_optimizer = GreedyOptimizer(english_sentence)\n",
    "greedy_translation = greedy_optimizer.optimize()[:-1]\n",
    "greedy_init = scorer.score_probabilities_for_each_word(english_tok, greedy_translation)\n",
    "greedy_init = torch.tensor(greedy_init[:-1, :].T)\n",
    "\n",
    "optimizer = ContinuousOptimizer(english_sentence)\n",
    "print('Optimization starts...')\n",
    "res = optimizer.optimize(init=greedy_init, method='multiplication', verbose=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy translation: ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁würde , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "Continuous translation: ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "Score of greedy translation =  tensor([0.5355])\n",
      "Score of continuous translation =  tensor([0.5352])\n"
     ]
    }
   ],
   "source": [
    "print('Greedy translation:', ' '.join(greedy_translation))\n",
    "print('Continuous translation:', ' '.join(res))\n",
    "print('Score of greedy translation = ' , -scorer.score_tokenized_texts([english_tok], [greedy_translation], method='multiplication', normalize=True))\n",
    "print('Score of continuous translation = ', -scorer.score_tokenized_texts([english_tok], [res], method='multiplication', normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU of greedy: 11.06\n",
      "BLEU of continuous: 9.67\n",
      "(measured by multi-bleu.perl script)\n"
     ]
    }
   ],
   "source": [
    "print('BLEU of greedy: 11.06')\n",
    "print('BLEU of continuous: 9.67')\n",
    "print('(measured by multi-bleu.perl script)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization starts...\n",
      "English sentence: But the victim's brother says he can't think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"\n",
      "Top 10 translations:\n",
      "1. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁indem ▁er ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \" [p = -20.19]\n",
      "2. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁wenn ▁er ▁sagte : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \" [p = -20.32]\n",
      "3. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁wenn ▁er ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \" [p = -20.34]\n",
      "4. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁indem ▁er ▁sagt : ▁\" Es ▁geht ▁ihm ▁endlich ▁gut . \" [p = -20.59]\n",
      "5. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁wenn ▁er ▁sagt : ▁\" Es ▁geht ▁ihm ▁endlich ▁gut . \" [p = -20.69]\n",
      "6. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁indem ▁er ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . ▁\" [p = -20.75]\n",
      "7. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁wenn ▁er ▁sagte : ▁\" Es ▁geht ▁ihm ▁endlich ▁gut . \" [p = -20.85]\n",
      "8. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁wenn ▁er ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . ▁\" [p = -20.88]\n",
      "9. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁wenn ▁er ▁sagte : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . ▁\" [p = -20.89]\n",
      "10. ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁nicht ▁an ▁jemand en ▁denken , ▁der ▁ihm ▁we h ▁tun ▁würde , ▁wenn ▁er ▁sagte : ▁\" Die ▁Dinge ▁würden ▁ihm ▁endlich ▁gut ▁gehen . \" [p = -21.83]\n",
      "Sanity check: log likehood of the best translation = -0.58\n"
     ]
    }
   ],
   "source": [
    "n_beams = 10\n",
    "optimizer = BeamOptimizer(english_sentence, n_beams=n_beams)\n",
    "print('Optimization starts...')\n",
    "translations, probabilities = optimizer.optimize()\n",
    "print('English sentence:', english_sentence)\n",
    "print('Top', n_beams, 'translations:')\n",
    "for i, translation in enumerate(translations):\n",
    "    print(f\"{i+1}. {' '.join(translation[:-1])} [p = {probabilities[i]:.2f}]\")\n",
    "\n",
    "score = scorer.score_tokenized_texts([english_tok], [translations[0][:-1]])[0]\n",
    "print(f'Sanity check: score of the best translation = {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU of beam: 12.98\n"
     ]
    }
   ],
   "source": [
    "print('BLEU of beam: 12.98')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentence = 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"'\n",
    "english_tok = tokenizer.tokenize(english_sentence)\n",
    "greedy_optimizer = GreedyOptimizer(english_sentence)\n",
    "greedy_translation = greedy_optimizer.optimize()[:-1]\n",
    "greedy_init = scorer.score_probabilities_for_each_word(english_tok, greedy_translation)\n",
    "greedy_init = torch.tensor(greedy_init[:-1, :].T)\n",
    "\n",
    "optimizer = ContinuousOptimizer(english_sentence)\n",
    "print('Optimization starts...')\n",
    "res = optimizer.optimize(init=greedy_init, method='cross_entropy', verbose=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b42835e0769a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menglish_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgreedy_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGreedyOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgreedy_translation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgreedy_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_probabilities_for_each_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_translation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgreedy_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/research/greedy_optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mnext_state_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_word_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish_tok_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/research/scorer.py\u001b[0m in \u001b[0;36mnext_word_probabilities\u001b[0;34m(self, english_tok_seq_gen, german_tok_seq_gen, relaxed)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msrc_data_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_tok_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menglish_tok_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_tok_seq_gen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtgt_data_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             batch_size=consts.OPT.batch_size,relaxed=relaxed)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore_probabilities_for_each_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_tok_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgerman_tok_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/translate/translator.py\u001b[0m in \u001b[0;36mnext_word_probabilities\u001b[0;34m(self, src_path, src_data_iter, tgt_path, tgt_data_iter, src_dir, batch_size, relaxed)\u001b[0m\n\u001b[1;32m    270\u001b[0m             sort_within_batch=True, shuffle=False)\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_next_word_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelaxed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     def translate(self,\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/translate/translator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    270\u001b[0m             sort_within_batch=True, shuffle=False)\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_next_word_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelaxed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     def translate(self,\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/translate/translator.py\u001b[0m in \u001b[0;36m_run_next_word_probabilities\u001b[0;34m(self, batch, data, relaxed)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;31m#  (1) run the encoder on the src\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0menc_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_bank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m         \u001b[0mdec_states\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_decoder_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_bank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/encoders/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, lengths)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# Run the forward pass of every layer of the tranformer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/encoders/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                     mask=mask)\n\u001b[1;32m     51\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/modules/position_ffn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "english_sentence = 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"'\n",
    "english_tok = tokenizer.tokenize(english_sentence)\n",
    "greedy_optimizer = GreedyOptimizer(english_sentence)\n",
    "greedy_translation = greedy_optimizer.optimize()[:-1]\n",
    "greedy_init = scorer.score_probabilities_for_each_word(english_tok, greedy_translation)\n",
    "greedy_init = torch.tensor(greedy_init[:-1, :].T)\n",
    "\n",
    "optimizer = ContinuousOptimizer(english_sentence)\n",
    "print('Optimization starts...')\n",
    "res = optimizer.optimize(init=greedy_init, method='multiplication')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy translation: ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁würde , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "Continuous translation: ▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagte , ▁er ▁könne ▁nicht en ▁denken , ▁der ▁ihn ▁verletz en ▁möchte , ▁und ▁sagte : ▁„ Es ▁ging ▁ihm ▁endlich ▁gut . \"\n",
      "Score of greedy translation =  tensor([-0.5355])\n",
      "Score of continuous translation =  tensor([-1.0102])\n"
     ]
    }
   ],
   "source": [
    "print('Greedy translation:', ' '.join(greedy_translation))\n",
    "print('Continuous translation:', ' '.join(res))\n",
    "print('Score of greedy translation = ' , scorer.score_tokenized_texts([english_tok], [greedy_translation]))\n",
    "print('Score of continuous translation = ', scorer.score_tokenized_texts([english_tok], [res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU of continuous translation with cross entropy: 13.46\n"
     ]
    }
   ],
   "source": [
    "print('BLEU of continuous translation with cross entropy: 13.46')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous optimization with cross entropy - uniform initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization starts...\n",
      "0\n",
      "Step 0 , loss score =  tensor([24.9637], grad_fn=<NegBackward>) max grad component =  lr =  50\n",
      "\t cscore =  tensor([24.9637], grad_fn=<NegBackward>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Dataset cannot contain Special Tokens",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1e6f343b7df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContinuousOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimization starts...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multiplication'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/research/continuous_optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, init, method, verbose, with_score)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tdscore = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_tokenized_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish_tok_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/research/scorer.py\u001b[0m in \u001b[0;36mscore_tokenized_texts\u001b[0;34m(self, english_tok_seq_gen, german_tok_seq_gen, relaxed, method, normalize)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                             \u001b[0mtgt_data_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                             relaxed=relaxed, method=method, normalize=normalize)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/translate/translator.py\u001b[0m in \u001b[0;36mscore_target\u001b[0;34m(self, src_path, src_data_iter, tgt_path, tgt_data_iter, src_dir, batch_size, relaxed, method, normalize)\u001b[0m\n\u001b[1;32m    174\u001b[0m                           \u001b[0muse_filter_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_filter_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                           \u001b[0mimage_channel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_channel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                           relaxed=relaxed)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/inputters/inputter.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(fields, data_type, src_data_iter, src_path, src_dir, tgt_data_iter, tgt_path, src_seq_length, tgt_seq_length, src_seq_length_trunc, tgt_seq_length_trunc, dynamic_dict, sample_rate, window_size, window_stride, window, normalize_audio, use_filter_pred, image_channel_size, relaxed)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mtgt_examples_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tgt_feats\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         TextDataset.make_text_examples_nfeats_tpl(\n\u001b[0;32m--> 242\u001b[0;31m             tgt_data_iter, tgt_path, tgt_seq_length_trunc, \"tgt\", relaxed=relaxed)\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/inputters/text_dataset.py\u001b[0m in \u001b[0;36mmake_text_examples_nfeats_tpl\u001b[0;34m(text_iter, text_path, truncate, side, relaxed)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mTextDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelaxed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelaxed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfirst_ex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_nfeats_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mnum_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_ex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/inputters/text_dataset.py\u001b[0m in \u001b[0;36mmake_examples\u001b[0;34m(text_iter, truncate, side, relaxed)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_feats\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                     \u001b[0mTextDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_text_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Documents/Studia/Semestr_7/Praca_Inzynierska/OpenNMT-py/onmt/inputters/dataset_base.py\u001b[0m in \u001b[0;36mextract_text_features\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0msplit_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"￨\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msplit_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspecial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;34m\"Dataset cannot contain Special Tokens\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msplit_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset cannot contain Special Tokens"
     ]
    }
   ],
   "source": [
    "english_sentence = 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"'\n",
    "english_tok = tokenizer.tokenize(english_sentence)\n",
    "optimizer = ContinuousOptimizer(english_sentence)\n",
    "print('Optimization starts...')\n",
    "res = optimizer.optimize(method='multiplication', verbose=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aligned_entropies(trans_i, trans_j):\n",
    "    plt.title('Entropy for each word in different translations (aligned)')\n",
    "    plt.xlabel('Word number')\n",
    "    plt.ylabel('Entropy')\n",
    "    \n",
    "    aligner = Aligner()\n",
    "    aligned_i, aligned_j = aligner.align(trans_i, trans_j)\n",
    "    \n",
    "    logprobs_i = scorer.score_probabilities_for_each_word(english_tok, trans_i)\n",
    "    probs_i = np.exp(logprobs_i)\n",
    "    logprobs_j = scorer.score_probabilities_for_each_word(english_tok, trans_j)\n",
    "    probs_j = np.exp(logprobs_j)\n",
    "\n",
    "    words = np.arange(len(aligned_i))\n",
    "    entropies_i = []\n",
    "    k = 0\n",
    "    for word in aligned_i:\n",
    "        if word == '[PLACEHOLDER]':\n",
    "            entropies_i += [-1.]\n",
    "        else:\n",
    "            entropies_i += [entropy(probs_i[k, :])]\n",
    "            k += 1\n",
    "    entropies_i = np.array(entropies_i)\n",
    "    \n",
    "    entropies_j = []\n",
    "    k = 0\n",
    "    for word in aligned_j:\n",
    "        if word == '[PLACEHOLDER]':\n",
    "            entropies_j += [-1.]\n",
    "        else:\n",
    "            entropies_j += [entropy(probs_j[k, :])]\n",
    "            k += 1\n",
    "    entropies_j = np.array(entropies_j)\n",
    "    \n",
    "    plt.scatter(words[entropies_i >= 0.], entropies_i[entropies_i >= 0.], label=f'Translation 1')\n",
    "    plt.scatter(words[entropies_j >= 0.], entropies_j[entropies_j >= 0.], label=f'Translation 2')\n",
    "    print(f'Translation 1:\\n{\" | \".join(aligned_i)}')\n",
    "    print(f'Translation 2:\\n{\" | \".join(aligned_j)}')\n",
    "    print()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 1:\n",
      "▁Aber | ▁der | ▁Bruder | ▁des | ▁Opfer | s | ▁sagt | , | ▁er | ▁könne | ▁nicht | ▁an | ▁jemand | en | ▁denken | , | ▁der | ▁ihm | ▁we | h | ▁tun | ▁würde | , | ▁indem | ▁er | ▁sagt | : | ▁\" | Es | ▁ging | ▁ihm | ▁endlich | ▁gut | . | \"\n",
      "Translation 2:\n",
      "▁Aber | ▁der | ▁Bruder | ▁des | ▁Opfer | s | ▁sagt | , | ▁er | ▁könne | [PLACEHOLDER] | [PLACEHOLDER] | ▁niemand | en | ▁denken | , | ▁der | [PLACEHOLDER] | ▁ihn | ▁verletz | en | ▁würde | , | [PLACEHOLDER] | ▁und | ▁sagt | : | ▁\" | Es | ▁ging | ▁ihm | ▁endlich | ▁gut | . | \"\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_aligned_entropies(translations[0][:-1], greedy_translation, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous with multiplication - more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WMT_DIR = '/home/pma/wmt/'\n",
    "german_path = WMT_DIR + 'test.de'\n",
    "english_path = WMT_DIR + 'test.en'\n",
    "\n",
    "def cstr(s, color='black'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "\n",
    "def load_test_data(path):\n",
    "    with open(path) as f:\n",
    "        return [tokenizer.detokenize(line.split()) for line in f.readlines()]\n",
    "\n",
    "def load_reference_data(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "    \n",
    "def perform_tests(k=50, initialization='greedy', beamsize=15):\n",
    "    aligner = Aligner()\n",
    "    english_sentences = load_test_data(english_path)\n",
    "    references = load_reference_data(german_path)\n",
    "    for english_sentence, reference in zip(english_sentences[:k], references):\n",
    "        english_tok = tokenizer.tokenize(english_sentence)\n",
    "        if initialization == 'greedy':\n",
    "            greedy_optimizer = GreedyOptimizer(english_sentence)\n",
    "            greedy_translation = greedy_optimizer.optimize()[:-1]\n",
    "        else:\n",
    "            greedy_optimizer = BeamOptimizer(english_sentence, beamsize)\n",
    "            greedy_translation = greedy_optimizer.optimize()[0][0][:-1]\n",
    "        \n",
    "        greedy_score = -scorer.score_tokenized_texts([english_tok], [greedy_translation], method='multiplication', normalize=True)\n",
    "        gbleu = bleu(reference, ' '.join(greedy_translation))\n",
    "\n",
    "        greedy_init = scorer.score_probabilities_for_each_word(english_tok, greedy_translation)\n",
    "        greedy_init = torch.tensor(greedy_init[:-1, :].T)\n",
    "        continuous_optimizer = ContinuousOptimizer(english_sentence)\n",
    "        continuous_translation, cscore = continuous_optimizer.optimize(init=greedy_init, method='multiplication', with_score=True, start_lr=100)\n",
    "        dscore = -scorer.score_tokenized_texts([english_tok], [continuous_translation], method='multiplication', normalize=True)\n",
    "        cbleu = bleu(reference, ' '.join(continuous_translation))\n",
    "\n",
    "        aligned_greedy, aligned_continuous = aligner.align(greedy_translation, continuous_translation)\n",
    "        \n",
    "        colored_continuous = [cstr(cword, color='black') if gword == cword else cstr(cword, color='green') \n",
    "            for gword, cword in zip(aligned_greedy, aligned_continuous)]\n",
    "        colored_continuous.append(cstr(f'[ cscore={cscore.item()}, dscore={dscore.item()}, bleu={cbleu} ]', color='black'))\n",
    "        \n",
    "        display(html_print(cstr(' '.join(aligned_greedy) + f'[ dscore={greedy_score.item()}, bleu={gbleu} ]', color='black')))\n",
    "        display(html_print(' '.join(colored_continuous)))\n",
    "        display(html_print('<br>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁28 - Jahr - O ld ▁Chef ▁Found ▁Dead[ dscore=0.5151540637016296, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁28</text> <text style=color:black>-</text> <text style=color:black>Jahr</text> <text style=color:black>-</text> <text style=color:black>O</text> <text style=color:black>ld</text> <text style=color:black>▁Chef</text> <text style=color:green>▁Ge</text> <text style=color:black>▁Dead</text> <text style=color:black>[ cscore=1.2482191324234009, dscore=1.1271077394485474, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Ein ▁28 - jährige r ▁Küchen chef , ▁der ▁kürz lich ▁nach ▁San ▁Francisco ▁ zog , ▁wurde ▁diese ▁Woche ▁im ▁Trepp en haus ▁einer ▁lokale n ▁Mal l ▁to t ▁auf gefunden .[ dscore=0.49215784668922424, bleu=31.29 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Ein</text> <text style=color:black>▁28</text> <text style=color:black>-</text> <text style=color:black>jährige</text> <text style=color:black>r</text> <text style=color:black>▁Küchen</text> <text style=color:black>chef</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:black>▁kürz</text> <text style=color:black>lich</text> <text style=color:black>▁nach</text> <text style=color:black>▁San</text> <text style=color:black>▁Francisco</text> <text style=color:black>▁</text> <text style=color:black>zog</text> <text style=color:black>,</text> <text style=color:black>▁wurde</text> <text style=color:black>▁diese</text> <text style=color:black>▁Woche</text> <text style=color:black>▁im</text> <text style=color:black>▁Trepp</text> <text style=color:black>en</text> <text style=color:black>haus</text> <text style=color:black>▁einer</text> <text style=color:black>▁lokale</text> <text style=color:black>n</text> <text style=color:black>▁Mal</text> <text style=color:black>l</text> <text style=color:black>▁to</text> <text style=color:black>t</text> <text style=color:black>▁auf</text> <text style=color:black>gefunden</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.5448085069656372, dscore=0.49215784668922424, bleu=31.29 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Aber ▁der ▁Bruder ▁des ▁Opfer s ▁sagt , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁ihn ▁verletz en ▁würde , ▁und ▁sagt : ▁\" Es ▁ging ▁ihm ▁endlich ▁gut . \"[ dscore=0.5355032086372375, bleu=11.46 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Aber</text> <text style=color:black>▁der</text> <text style=color:black>▁Bruder</text> <text style=color:black>▁des</text> <text style=color:black>▁Opfer</text> <text style=color:black>s</text> <text style=color:black>▁sagt</text> <text style=color:black>,</text> <text style=color:black>▁er</text> <text style=color:black>▁könne</text> <text style=color:black>▁niemand</text> <text style=color:black>en</text> <text style=color:black>▁denken</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:black>▁ihn</text> <text style=color:black>▁verletz</text> <text style=color:black>en</text> <text style=color:green>▁möchte</text> <text style=color:black>,</text> <text style=color:black>▁und</text> <text style=color:black>▁sagt</text> <text style=color:black>:</text> <text style=color:black>▁\"</text> <text style=color:black>Es</text> <text style=color:black>▁ging</text> <text style=color:black>▁ihm</text> <text style=color:black>▁endlich</text> <text style=color:black>▁gut</text> <text style=color:black>.</text> <text style=color:black>\"</text> <text style=color:black>[ cscore=1.581463098526001, dscore=0.5352103114128113, bleu=10.77 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die ▁Körper ▁in ▁der ▁West field ▁Mal l ▁Mittwoch ▁Morgen ▁wurde ▁als ▁28 - jährige r ▁in ▁San ▁Francisco ▁Wohn sitz ▁Frank ▁Galicia ▁identifi ziert , ▁sagte ▁der ▁San ▁Francisco ▁Medical ▁Ex ami ner ' s ▁Office .[ dscore=0.418877512216568, bleu=20.01 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die</text> <text style=color:black>▁Körper</text> <text style=color:black>▁in</text> <text style=color:black>▁der</text> <text style=color:black>▁West</text> <text style=color:black>field</text> <text style=color:black>▁Mal</text> <text style=color:black>l</text> <text style=color:black>▁Mittwoch</text> <text style=color:black>▁Morgen</text> <text style=color:black>▁wurde</text> <text style=color:black>▁als</text> <text style=color:black>▁28</text> <text style=color:black>-</text> <text style=color:green>J</text> <text style=color:green>▁in</text> <text style=color:black>▁in</text> <text style=color:black>▁San</text> <text style=color:black>▁Francisco</text> <text style=color:black>▁Wohn</text> <text style=color:black>sitz</text> <text style=color:black>▁Frank</text> <text style=color:black>▁Galicia</text> <text style=color:black>▁identifi</text> <text style=color:black>ziert</text> <text style=color:black>,</text> <text style=color:black>▁sagte</text> <text style=color:black>▁der</text> <text style=color:black>▁San</text> <text style=color:black>▁Francisco</text> <text style=color:black>▁Medical</text> <text style=color:black>▁Ex</text> <text style=color:black>ami</text> <text style=color:black>ner</text> <text style=color:black>'</text> <text style=color:black>s</text> <text style=color:black>▁Office</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.6285784244537354, dscore=1.1574435234069824, bleu=20.01 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die ▁Polizei abteilung ▁von ▁San ▁Francisco ▁sagte , ▁dass ▁der ▁Tod ▁eines ▁Mord es ▁be urteil t ▁wurde ▁und ▁eine ▁Untersuchung ▁an hält .[ dscore=0.7827282547950745, bleu=20.29 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die</text> <text style=color:black>▁Polizei</text> <text style=color:black>abteilung</text> <text style=color:black>▁von</text> <text style=color:black>▁San</text> <text style=color:black>▁Francisco</text> <text style=color:black>▁sagte</text> <text style=color:black>,</text> <text style=color:black>▁dass</text> <text style=color:black>▁der</text> <text style=color:black>▁Tod</text> <text style=color:black>▁eines</text> <text style=color:black>▁Mord</text> <text style=color:black>es</text> <text style=color:black>▁be</text> <text style=color:black>urteil</text> <text style=color:black>t</text> <text style=color:black>▁wurde</text> <text style=color:black>▁und</text> <text style=color:black>▁eine</text> <text style=color:black>▁Untersuchung</text> <text style=color:black>▁an</text> <text style=color:black>hält</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.965139389038086, dscore=0.7827282547950745, bleu=20.29 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der ▁Bruder ▁des ▁Opfer s , ▁Louis ▁Galicia , ▁sagte ▁der ▁A BC - Station ▁K GO ▁in ▁San ▁Francisco , ▁dass ▁Frank , ▁der ▁zuvor ▁in ▁Boston ▁als ▁Linien koch ▁tätig ▁war , ▁vor ▁sechs ▁Monaten ▁seinen ▁Traum job ▁als ▁Küchen chef ▁im ▁Restaurant ▁Son s ▁& ▁D aught ers ▁in ▁San ▁Francisco ▁ge land et ▁hatte .[ dscore=0.47621670365333557, bleu=48.5 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der</text> <text style=color:black>▁Bruder</text> <text style=color:black>▁des</text> <text style=color:black>▁Opfer</text> <text style=color:black>s</text> <text style=color:black>,</text> <text style=color:black>▁Louis</text> <text style=color:black>▁Galicia</text> <text style=color:black>,</text> <text style=color:black>▁sagte</text> <text style=color:green>▁in</text> <text style=color:black>▁A</text> <text style=color:black>BC</text> <text style=color:black>-</text> <text style=color:black>Station</text> <text style=color:black>▁K</text> <text style=color:black>GO</text> <text style=color:black>▁in</text> <text style=color:black>▁San</text> <text style=color:black>▁Francisco</text> <text style=color:black>,</text> <text style=color:black>▁dass</text> <text style=color:black>▁Frank</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:black>▁zuvor</text> <text style=color:black>▁in</text> <text style=color:black>▁Boston</text> <text style=color:black>▁als</text> <text style=color:black>▁Linien</text> <text style=color:black>koch</text> <text style=color:black>▁tätig</text> <text style=color:black>▁war</text> <text style=color:black>,</text> <text style=color:black>▁vor</text> <text style=color:black>▁sechs</text> <text style=color:black>▁Monaten</text> <text style=color:black>▁seinen</text> <text style=color:black>▁Traum</text> <text style=color:black>job</text> <text style=color:black>▁als</text> <text style=color:black>▁Küchen</text> <text style=color:black>chef</text> <text style=color:black>▁im</text> <text style=color:black>▁Restaurant</text> <text style=color:black>▁Son</text> <text style=color:black>s</text> <text style=color:black>▁&</text> <text style=color:black>▁D</text> <text style=color:black>aught</text> <text style=color:black>ers</text> <text style=color:black>▁in</text> <text style=color:black>▁San</text> <text style=color:black>▁Francisco</text> <text style=color:black>▁ge</text> <text style=color:black>land</text> <text style=color:black>et</text> <text style=color:black>▁hatte</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.6336970329284668, dscore=0.6517391800880432, bleu=48.5 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Ein ▁Sprech er ▁für ▁Söhne ▁und ▁Tö chter ▁sagte , ▁dass ▁sie ▁durch ▁seinen ▁Tod ▁\" ver spot tet ▁und ▁verw üstet \" ▁seien .[ dscore=0.4737871289253235, bleu=17.15 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Ein</text> <text style=color:black>▁Sprech</text> <text style=color:black>er</text> <text style=color:black>▁für</text> <text style=color:black>▁Söhne</text> <text style=color:green>▁&</text> <text style=color:black>▁Tö</text> <text style=color:black>chter</text> <text style=color:black>▁sagte</text> <text style=color:black>,</text> <text style=color:black>▁dass</text> <text style=color:black>▁sie</text> <text style=color:black>▁durch</text> <text style=color:black>▁seinen</text> <text style=color:black>▁Tod</text> <text style=color:green>▁„</text> <text style=color:black>ver</text> <text style=color:black>spot</text> <text style=color:black>tet</text> <text style=color:black>▁und</text> <text style=color:black>▁verw</text> <text style=color:black>üstet</text> <text style=color:black>\"</text> <text style=color:black>▁seien</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.331511378288269, dscore=0.6347835659980774, bleu=15.35 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Wir ▁sind ▁ein ▁kleines ▁Team , ▁das ▁wie ▁eine ▁enge ▁k n it ▁Familie ▁arbeitet ▁und ▁er ▁wird ▁sehr ▁vermiss t ▁werden \", ▁sagte ▁der ▁Sprech er .[ dscore=0.4113239645957947, bleu=48.83 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Wir</text> <text style=color:black>▁sind</text> <text style=color:black>▁ein</text> <text style=color:black>▁kleines</text> <text style=color:black>▁Team</text> <text style=color:black>,</text> <text style=color:black>▁das</text> <text style=color:black>▁wie</text> <text style=color:black>▁eine</text> <text style=color:black>▁enge</text> <text style=color:black>▁k</text> <text style=color:black>n</text> <text style=color:black>it</text> <text style=color:black>▁Familie</text> <text style=color:green>▁funktioniert</text> <text style=color:black>▁und</text> <text style=color:black>▁er</text> <text style=color:black>▁wird</text> <text style=color:black>▁sehr</text> <text style=color:black>▁vermiss</text> <text style=color:black>t</text> <text style=color:black>▁werden</text> <text style=color:black>\",</text> <text style=color:black>▁sagte</text> <text style=color:black>▁der</text> <text style=color:black>▁Sprech</text> <text style=color:black>er</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.6050571203231812, dscore=0.4085864722728729, bleu=45.51 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Unsere ▁Gedanken ▁und ▁unser ▁Bei leid ▁sind ▁bei ▁Frank s ▁Familie ▁und ▁Freunden ▁in ▁dieser ▁schwierig en ▁Zeit .[ dscore=0.5129178166389465, bleu=65.99 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Unsere</text> <text style=color:green>▁Gefühl</text> <text style=color:black>▁und</text> <text style=color:black>▁unser</text> <text style=color:black>▁Bei</text> <text style=color:black>leid</text> <text style=color:black>▁sind</text> <text style=color:black>▁bei</text> <text style=color:black>▁Frank</text> <text style=color:black>s</text> <text style=color:black>▁Familie</text> <text style=color:black>▁und</text> <text style=color:black>▁Freunden</text> <text style=color:black>▁in</text> <text style=color:black>▁dieser</text> <text style=color:black>▁schwierig</text> <text style=color:black>en</text> <text style=color:black>▁Zeit</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.4884217977523804, dscore=0.9111337661743164, bleu=53.82 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Louis ▁Galicia ▁sagte ▁Frank ▁zu nächst ▁in ▁den ▁Herberge n , ▁aber ▁vor ▁kurze m : ▁„ Die ▁Dinge ▁ klapp ten ▁ihm ▁endlich ▁gut . \"[ dscore=0.7988974452018738, bleu=12.5 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Louis</text> <text style=color:black>▁Galicia</text> <text style=color:black>▁sagte</text> <text style=color:black>▁Frank</text> <text style=color:black>▁zu</text> <text style=color:black>nächst</text> <text style=color:black>▁in</text> <text style=color:black>▁den</text> <text style=color:black>▁Herberge</text> <text style=color:black>n</text> <text style=color:black>,</text> <text style=color:black>▁aber</text> <text style=color:black>▁vor</text> <text style=color:black>▁kurze</text> <text style=color:black>m</text> <text style=color:black>:</text> <text style=color:black>▁„</text> <text style=color:black>Die</text> <text style=color:black>▁Dinge</text> <text style=color:green>▁gehen</text> <text style=color:black>klapp</text> <text style=color:black>ten</text> <text style=color:black>▁ihm</text> <text style=color:black>▁endlich</text> <text style=color:black>▁gut</text> <text style=color:black>.</text> <text style=color:black>\"</text> <text style=color:black>[ cscore=1.9759684801101685, dscore=1.3465930223464966, bleu=12.23 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Er ▁fand ▁eine ▁Wohnung , ▁er ▁war ▁ein ▁Mädchen , \" ▁Louis ▁Gal izi en ▁sagte ▁K GO .[ dscore=0.5281389355659485, bleu=34.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Er</text> <text style=color:black>▁fand</text> <text style=color:black>▁eine</text> <text style=color:black>▁Wohnung</text> <text style=color:black>,</text> <text style=color:black>▁er</text> <text style=color:black>▁war</text> <text style=color:black>▁ein</text> <text style=color:black>▁Mädchen</text> <text style=color:black>,</text> <text style=color:black>\"</text> <text style=color:black>▁Louis</text> <text style=color:black>▁Gal</text> <text style=color:black>izi</text> <text style=color:black>en</text> <text style=color:black>▁sagte</text> <text style=color:black>▁K</text> <text style=color:black>GO</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.1716722249984741, dscore=0.5281389355659485, bleu=34.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Louis ▁Galicia ▁sagte , ▁er ▁könne ▁niemand en ▁denken , ▁der ▁seinen ▁jüng er en ▁Bruder ▁verletz en ▁möchte .[ dscore=0.45707157254219055, bleu=26.73 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Louis</text> <text style=color:black>▁Galicia</text> <text style=color:black>▁sagte</text> <text style=color:black>,</text> <text style=color:black>▁er</text> <text style=color:black>▁könne</text> <text style=color:black>▁niemand</text> <text style=color:black>en</text> <text style=color:black>▁denken</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:black>▁seinen</text> <text style=color:black>▁jüng</text> <text style=color:black>er</text> <text style=color:black>en</text> <text style=color:black>▁Bruder</text> <text style=color:black>▁verletz</text> <text style=color:black>en</text> <text style=color:black>▁möchte</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.7214127779006958, dscore=0.45707157254219055, bleu=26.73 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er ▁war ▁ein ▁freundlich er ▁Geist ▁mit ▁einem ▁großen ▁Herz .[ dscore=0.40280836820602417, bleu=52.54 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er</text> <text style=color:black>▁war</text> <text style=color:black>▁ein</text> <text style=color:black>▁freundlich</text> <text style=color:black>er</text> <text style=color:black>▁Geist</text> <text style=color:black>▁mit</text> <text style=color:black>▁einem</text> <text style=color:black>▁großen</text> <text style=color:black>▁Herz</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.9705040454864502, dscore=0.40280836820602417, bleu=52.54 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Seine ▁Verbindung ▁mit ▁der ▁Familie ▁machte ▁uns ▁immer ▁zu ▁einem ▁Gericht , ▁das ▁uns ▁zum ▁Abendessen ▁machte \", ▁sagte ▁Louis ▁Galici en .[ dscore=0.520112931728363, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Seine</text> <text style=color:black>▁Verbindung</text> <text style=color:black>▁mit</text> <text style=color:black>▁der</text> <text style=color:black>▁Familie</text> <text style=color:green>▁hat</text> <text style=color:black>▁uns</text> <text style=color:black>▁immer</text> <text style=color:black>▁zu</text> <text style=color:black>▁einem</text> <text style=color:black>▁Gericht</text> <text style=color:black>,</text> <text style=color:black>▁das</text> <text style=color:black>▁uns</text> <text style=color:black>▁zum</text> <text style=color:black>▁Abendessen</text> <text style=color:black>▁machte</text> <text style=color:black>\",</text> <text style=color:black>▁sagte</text> <text style=color:black>▁Louis</text> <text style=color:black>▁Galici</text> <text style=color:black>en</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.513554334640503, dscore=0.8294508457183838, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er ▁wollte ▁nie ▁in ▁irgendeine r ▁Weise ▁in ▁irgendeine r ▁Weise ▁von ▁Veränderungen ▁sein .[ dscore=1.0596872568130493, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er</text> <text style=color:black>▁wollte</text> <text style=color:black>▁nie</text> <text style=color:black>▁in</text> <text style=color:black>▁irgendeine</text> <text style=color:black>r</text> <text style=color:black>▁Weise</text> <text style=color:black>▁in</text> <text style=color:black>▁irgendeine</text> <text style=color:black>r</text> <text style=color:green>▁Art</text> <text style=color:black>▁von</text> <text style=color:green>▁Veränderung</text> <text style=color:green>▁leben</text> <text style=color:black>.</text> <text style=color:black>[ cscore=2.3171253204345703, dscore=1.2011477947235107, bleu=25.83 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er ▁war ▁der ▁Bruder , ▁der ▁mit ▁dem ▁Fluss ▁ging .[ dscore=0.3537465035915375, bleu=61.32 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er</text> <text style=color:black>▁war</text> <text style=color:black>▁der</text> <text style=color:black>▁Bruder</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:black>▁mit</text> <text style=color:black>▁dem</text> <text style=color:black>▁Fluss</text> <text style=color:black>▁ging</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.6226787567138672, dscore=0.3537465035915375, bleu=61.32 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Mit ▁allem , ▁was ▁mit ▁der ▁Welt ▁schief ▁läuft , ▁war ▁er ▁der ▁Diamant ▁in ▁der ▁rau en , ▁die ▁jeden ▁Tag ▁ strahl end ▁schien \", ▁sagte ▁er .[ dscore=0.5464362502098083, bleu=28.31 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Mit</text> <text style=color:black>▁allem</text> <text style=color:black>,</text> <text style=color:black>▁was</text> <text style=color:black>▁mit</text> <text style=color:black>▁der</text> <text style=color:black>▁Welt</text> <text style=color:black>▁schief</text> <text style=color:black>▁läuft</text> <text style=color:black>,</text> <text style=color:black>▁war</text> <text style=color:black>▁er</text> <text style=color:green>▁jene</text> <text style=color:black>▁Diamant</text> <text style=color:black>▁in</text> <text style=color:black>▁der</text> <text style=color:green>▁grob</text> <text style=color:black>en</text> <text style=color:green>▁Maschine</text> <text style=color:black>▁die</text> <text style=color:black>▁jeden</text> <text style=color:black>▁Tag</text> <text style=color:black>▁</text> <text style=color:black>strahl</text> <text style=color:black>end</text> <text style=color:black>▁schien</text> <text style=color:black>\",</text> <text style=color:black>▁sagte</text> <text style=color:black>▁er</text> <text style=color:black>.</text> <text style=color:black>[ cscore=2.0391130447387695, dscore=0.9967603087425232, bleu=27.97 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Jeder , ▁der ▁Informationen ▁hat , ▁wird ▁gebe ten , ▁die ▁S F PD - T ipp - Linie ▁unter ▁4 15 -5 75 - 44 44 ▁anzu rufen .[ dscore=0.529942512512207, bleu=49.06 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Jeder</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:green>▁über</text> <text style=color:black>▁hat</text> <text style=color:black>,</text> <text style=color:black>▁wird</text> <text style=color:black>▁gebe</text> <text style=color:black>ten</text> <text style=color:black>,</text> <text style=color:black>▁die</text> <text style=color:black>▁S</text> <text style=color:black>F</text> <text style=color:black>PD</text> <text style=color:black>-</text> <text style=color:black>T</text> <text style=color:black>ipp</text> <text style=color:black>-</text> <text style=color:black>Linie</text> <text style=color:black>▁unter</text> <text style=color:black>▁4</text> <text style=color:black>15</text> <text style=color:black>-5</text> <text style=color:black>75</text> <text style=color:black>-</text> <text style=color:black>44</text> <text style=color:black>44</text> <text style=color:black>▁anzu</text> <text style=color:black>rufen</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.8608603477478027, dscore=1.1606014966964722, bleu=45.46 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁J en ni fer ▁An ist on : ▁Ich ▁bin ▁immer ▁mit ▁P ig e on en ▁gef ütter t .[ dscore=0.8773913383483887, bleu=41.01 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁J</text> <text style=color:black>en</text> <text style=color:black>ni</text> <text style=color:black>fer</text> <text style=color:black>▁An</text> <text style=color:black>ist</text> <text style=color:black>on</text> <text style=color:black>:</text> <text style=color:black>▁Ich</text> <text style=color:black>▁bin</text> <text style=color:black>▁immer</text> <text style=color:black>▁mit</text> <text style=color:black>▁P</text> <text style=color:black>ig</text> <text style=color:black>e</text> <text style=color:black>on</text> <text style=color:black>en</text> <text style=color:green>g</text> <text style=color:black>ütter</text> <text style=color:black>t</text> <text style=color:black>.</text> <text style=color:black>[ cscore=2.204559326171875, dscore=1.9460837841033936, bleu=41.01 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁J en ni fer ▁An ist on ▁muss ▁nicht ▁immer ▁perfekt ▁oder ▁erfolgreich ▁sein .[ dscore=0.1678551286458969, bleu=100.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁J</text> <text style=color:black>en</text> <text style=color:black>ni</text> <text style=color:black>fer</text> <text style=color:black>▁An</text> <text style=color:black>ist</text> <text style=color:black>on</text> <text style=color:black>▁muss</text> <text style=color:black>▁nicht</text> <text style=color:black>▁immer</text> <text style=color:black>▁perfekt</text> <text style=color:black>▁oder</text> <text style=color:black>▁erfolgreich</text> <text style=color:black>▁sein</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.1885985136032104, dscore=0.1678551286458969, bleu=100.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Das ▁hat ▁der ▁Hollywood - Star ▁in ▁einem ▁Interview ▁mehr ▁als ▁deutlich ▁gemacht .[ dscore=0.3610643446445465, bleu=61.15 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Das</text> <text style=color:black>▁hat</text> <text style=color:black>▁der</text> <text style=color:black>▁Hollywood</text> <text style=color:black>-</text> <text style=color:black>Star</text> <text style=color:black>▁in</text> <text style=color:black>▁einem</text> <text style=color:black>▁Interview</text> <text style=color:black>▁mehr</text> <text style=color:black>▁als</text> <text style=color:black>▁deutlich</text> <text style=color:black>▁gemacht</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.600045919418335, dscore=0.3610643446445465, bleu=61.15 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Ich ▁bin ▁immer ▁mit ▁einem ▁P ig ment ▁be kleide t ▁oder ▁von ▁jemand em ▁gekennzeichnet , ▁der , ▁wenn ▁überhaupt , ▁wenig ▁mit ▁mir ▁und ▁der ▁Realität ▁zu ▁tun ▁hat . ▁\"[ dscore=0.7506160736083984, bleu=15.06 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Ich</text> <text style=color:black>▁bin</text> <text style=color:black>▁immer</text> <text style=color:black>▁mit</text> <text style=color:green>▁P</text> <text style=color:black>▁P</text> <text style=color:black>ig</text> <text style=color:green>eu</text> <text style=color:black>▁be</text> <text style=color:black>kleide</text> <text style=color:black>t</text> <text style=color:black>▁oder</text> <text style=color:black>▁von</text> <text style=color:black>▁jemand</text> <text style=color:black>em</text> <text style=color:black>▁gekennzeichnet</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:black>,</text> <text style=color:black>▁wenn</text> <text style=color:black>▁überhaupt</text> <text style=color:black>,</text> <text style=color:black>▁wenig</text> <text style=color:black>▁mit</text> <text style=color:black>▁mir</text> <text style=color:black>▁und</text> <text style=color:black>▁der</text> <text style=color:black>▁Realität</text> <text style=color:black>▁zu</text> <text style=color:black>▁tun</text> <text style=color:black>▁hat</text> <text style=color:black>.</text> <text style=color:black>▁\"</text> <text style=color:black>[ cscore=2.384376049041748, dscore=1.3915789127349854, bleu=15.06 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Per s ö n lich ▁habe ▁ich ▁keine n ▁Wunsch , ▁immer ▁perfekt ▁und ▁erfolgreich ▁zu ▁sein . ▁\"[ dscore=0.48416948318481445, bleu=41.53 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Per</text> <text style=color:black>s</text> <text style=color:black>ö</text> <text style=color:black>n</text> <text style=color:black>lich</text> <text style=color:black>▁habe</text> <text style=color:black>▁ich</text> <text style=color:black>▁keine</text> <text style=color:black>n</text> <text style=color:black>▁Wunsch</text> <text style=color:black>,</text> <text style=color:black>▁immer</text> <text style=color:black>▁perfekt</text> <text style=color:black>▁und</text> <text style=color:black>▁erfolgreich</text> <text style=color:black>▁zu</text> <text style=color:black>▁sein</text> <text style=color:black>.</text> <text style=color:green>\"</text> <text style=color:black>[ cscore=1.5339939594268799, dscore=0.4863690137863159, bleu=44.39 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Um ▁ehrlich ▁zu ▁sein , ▁gebe ▁ich ▁keine n ▁wirklich en ▁Ver damm nis ▁über ▁solche ▁Dinge . \"[ dscore=0.5508469343185425, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Um</text> <text style=color:black>▁ehrlich</text> <text style=color:black>▁zu</text> <text style=color:black>▁sein</text> <text style=color:black>,</text> <text style=color:black>▁gebe</text> <text style=color:black>▁ich</text> <text style=color:black>▁keine</text> <text style=color:black>n</text> <text style=color:black>▁wirklich</text> <text style=color:black>en</text> <text style=color:black>▁Ver</text> <text style=color:black>damm</text> <text style=color:black>nis</text> <text style=color:black>▁über</text> <text style=color:black>▁solche</text> <text style=color:black>▁Dinge</text> <text style=color:black>.</text> <text style=color:black>\"</text> <text style=color:black>[ cscore=1.3480722904205322, dscore=0.5508469343185425, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁J en ni fer ▁An ist on ▁( 47 ) ▁kann ▁nur ▁la chen , ▁dass ▁sie ▁in ▁der ▁aktuelle n ▁Ausgabe ▁von ▁\" K osmopolit an \" ▁als ▁\" Mi s s ▁Perfect \" ▁bezeichnet ▁wurde .[ dscore=0.23542645573616028, bleu=63.61 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁J</text> <text style=color:black>en</text> <text style=color:black>ni</text> <text style=color:black>fer</text> <text style=color:black>▁An</text> <text style=color:black>ist</text> <text style=color:black>on</text> <text style=color:black>▁(</text> <text style=color:black>47</text> <text style=color:black>)</text> <text style=color:black>▁kann</text> <text style=color:black>▁nur</text> <text style=color:black>▁la</text> <text style=color:black>chen</text> <text style=color:black>,</text> <text style=color:black>▁dass</text> <text style=color:black>▁sie</text> <text style=color:black>▁in</text> <text style=color:black>▁der</text> <text style=color:black>▁aktuelle</text> <text style=color:black>n</text> <text style=color:black>▁Ausgabe</text> <text style=color:black>▁von</text> <text style=color:black>▁\"</text> <text style=color:black>K</text> <text style=color:black>osmopolit</text> <text style=color:black>an</text> <text style=color:black>\"</text> <text style=color:black>▁als</text> <text style=color:black>▁\"</text> <text style=color:black>Mi</text> <text style=color:black>s</text> <text style=color:black>s</text> <text style=color:black>▁Perfect</text> <text style=color:black>\"</text> <text style=color:black>▁bezeichnet</text> <text style=color:black>▁wurde</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.519295334815979, dscore=0.23542645573616028, bleu=63.61 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die ▁Schauspieler in ▁ zieht ▁es ▁vor , ▁ihre ▁Zeit ▁zu ▁Hause ▁vor ▁dem ▁Fernsehen ▁mit ▁ihre m ▁Haar ▁in ▁einem ▁entspannt en ▁Bu n ▁zu ▁verbringen , ▁essen ▁mex ikanische ▁Lebensmittel , ▁berichtet ▁die ▁Zeitschrift .[ dscore=0.5059648752212524, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>▁Sie</text> <text style=color:black>▁Schauspieler</text> <text style=color:black>in</text> <text style=color:black>▁</text> <text style=color:black>zieht</text> <text style=color:black>▁es</text> <text style=color:black>▁vor</text> <text style=color:black>,</text> <text style=color:black>▁ihre</text> <text style=color:black>▁Zeit</text> <text style=color:black>▁zu</text> <text style=color:black>▁Hause</text> <text style=color:black>▁vor</text> <text style=color:black>▁dem</text> <text style=color:black>▁Fernsehen</text> <text style=color:black>▁mit</text> <text style=color:black>▁ihre</text> <text style=color:black>m</text> <text style=color:black>▁Haar</text> <text style=color:black>▁in</text> <text style=color:black>▁einem</text> <text style=color:black>▁entspannt</text> <text style=color:black>en</text> <text style=color:green>▁bu</text> <text style=color:black>n</text> <text style=color:black>▁zu</text> <text style=color:black>▁verbringen</text> <text style=color:black>,</text> <text style=color:black>▁essen</text> <text style=color:black>▁mex</text> <text style=color:black>ikanische</text> <text style=color:black>▁Lebensmittel</text> <text style=color:black>,</text> <text style=color:black>▁berichtet</text> <text style=color:black>▁die</text> <text style=color:black>▁Zeitschrift</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.9639503955841064, dscore=0.9045822024345398, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die ▁Frage , ▁ob ▁sie ▁Kinder ▁haben ▁möchte , ▁ist ▁ihre ▁Lü ge : ▁\" Wir ▁Frauen ▁brauchen ▁nicht ▁verhe irate t ▁zu ▁werden ▁oder ▁Kinder ▁zu ▁haben , ▁um ▁sich ▁erfüllt ▁zu ▁fühl en \", ▁sagt ▁der ▁Hollywood - Star .[ dscore=0.6018204689025879, bleu=22.27 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die</text> <text style=color:black>▁Frage</text> <text style=color:black>,</text> <text style=color:black>▁ob</text> <text style=color:black>▁sie</text> <text style=color:black>▁Kinder</text> <text style=color:black>▁haben</text> <text style=color:black>▁möchte</text> <text style=color:black>,</text> <text style=color:black>▁ist</text> <text style=color:green>▁</text> <text style=color:black>▁Lü</text> <text style=color:black>ge</text> <text style=color:black>:</text> <text style=color:black>▁\"</text> <text style=color:black>Wir</text> <text style=color:black>▁Frauen</text> <text style=color:black>▁brauchen</text> <text style=color:black>▁nicht</text> <text style=color:black>▁verhe</text> <text style=color:black>irate</text> <text style=color:black>t</text> <text style=color:black>▁zu</text> <text style=color:green>▁sein</text> <text style=color:black>▁oder</text> <text style=color:black>▁Kinder</text> <text style=color:black>▁zu</text> <text style=color:black>▁haben</text> <text style=color:black>,</text> <text style=color:black>▁um</text> <text style=color:black>▁sich</text> <text style=color:black>▁erfüllt</text> <text style=color:black>▁zu</text> <text style=color:black>▁fühl</text> <text style=color:black>en</text> <text style=color:black>\",</text> <text style=color:black>▁sagt</text> <text style=color:black>▁der</text> <text style=color:black>▁Hollywood</text> <text style=color:black>-</text> <text style=color:black>Star</text> <text style=color:black>.</text> <text style=color:black>[ cscore=2.185471296310425, dscore=0.8382052183151245, bleu=24.13 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁In ▁ihre m ▁neue n ▁Film ▁\" M other ' s ▁Day ▁- ▁love ▁is n ' t ▁child ' s ▁play \" ▁ schlägt ▁An ist on ▁als ▁eine ▁allein er ziehen de ▁Mutter ▁mit ▁zwei ▁Söhne n .[ dscore=0.39282190799713135, bleu=54.6 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁In</text> <text style=color:black>▁ihre</text> <text style=color:black>m</text> <text style=color:black>▁neue</text> <text style=color:black>n</text> <text style=color:black>▁Film</text> <text style=color:black>▁\"</text> <text style=color:black>M</text> <text style=color:black>other</text> <text style=color:black>'</text> <text style=color:black>s</text> <text style=color:black>▁Day</text> <text style=color:black>▁-</text> <text style=color:green>▁Love</text> <text style=color:black>▁is</text> <text style=color:black>n</text> <text style=color:black>'</text> <text style=color:black>t</text> <text style=color:black>▁child</text> <text style=color:black>'</text> <text style=color:black>s</text> <text style=color:black>▁play</text> <text style=color:green>▁\"</text> <text style=color:black>▁</text> <text style=color:black>schlägt</text> <text style=color:black>▁An</text> <text style=color:black>ist</text> <text style=color:black>on</text> <text style=color:black>▁als</text> <text style=color:black>▁eine</text> <text style=color:black>▁allein</text> <text style=color:green>▁er</text> <text style=color:black>ziehen</text> <text style=color:black>de</text> <text style=color:black>▁Mutter</text> <text style=color:black>▁mit</text> <text style=color:black>▁zwei</text> <text style=color:black>▁Söhne</text> <text style=color:black>n</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.3453096151351929, dscore=0.5531466007232666, bleu=46.75 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der ▁Film ▁wird ▁am ▁25. ▁August ▁in ▁Deutschland ▁veröffentlicht .[ dscore=0.3767436146736145, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der</text> <text style=color:black>▁Film</text> <text style=color:black>▁wird</text> <text style=color:black>▁am</text> <text style=color:black>▁25.</text> <text style=color:black>▁August</text> <text style=color:black>▁in</text> <text style=color:black>▁Deutschland</text> <text style=color:black>▁veröffentlicht</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.5540610551834106, dscore=0.3767436146736145, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Golf er ▁Lang er ▁wird ▁mit ▁der ▁Sport ▁Py ram ide ▁ausgezeichnet .[ dscore=0.4223611056804657, bleu=23.8 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Golf</text> <text style=color:black>er</text> <text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:black>▁wird</text> <text style=color:green>▁von</text> <text style=color:black>▁der</text> <text style=color:black>▁Sport</text> <text style=color:black>▁Py</text> <text style=color:black>ram</text> <text style=color:black>ide</text> <text style=color:black>▁ausgezeichnet</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.7944890260696411, dscore=0.6960044503211975, bleu=23.8 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Seine ▁Erfahrung en ▁auf ▁dem ▁Pferd ▁sind ▁vernachlässig bar .[ dscore=0.5811247825622559, bleu=66.06 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Seine</text> <text style=color:black>▁Erfahrung</text> <text style=color:black>en</text> <text style=color:black>▁auf</text> <text style=color:black>▁dem</text> <text style=color:black>▁Pferd</text> <text style=color:green>▁zu</text> <text style=color:black>▁vernachlässig</text> <text style=color:black>bar</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.8234646320343018, dscore=2.4308128356933594, bleu=53.73 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Am ▁Ende ▁eines ▁Golf t urnier s ▁in ▁Südafrika ▁ fuhr en ▁Bernhard ▁Lang er ▁und ▁ein ▁Kolleg e ▁für ▁ein ▁wenig ▁Spaß ▁am ▁Strand ▁entlang .[ dscore=0.3473149836063385, bleu=37.03 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Am</text> <text style=color:black>▁Ende</text> <text style=color:black>▁eines</text> <text style=color:black>▁Golf</text> <text style=color:black>t</text> <text style=color:black>urnier</text> <text style=color:black>s</text> <text style=color:black>▁in</text> <text style=color:black>▁Südafrika</text> <text style=color:black>▁</text> <text style=color:black>fuhr</text> <text style=color:black>en</text> <text style=color:black>▁Bernhard</text> <text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:black>▁und</text> <text style=color:black>▁ein</text> <text style=color:black>▁Kolleg</text> <text style=color:black>e</text> <text style=color:black>▁für</text> <text style=color:black>▁ein</text> <text style=color:black>▁wenig</text> <text style=color:black>▁Spaß</text> <text style=color:black>▁am</text> <text style=color:black>▁Strand</text> <text style=color:black>▁entlang</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.3445950746536255, dscore=0.3473149836063385, bleu=37.03 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Es ▁war ▁ein ▁erst er ▁für ▁den ▁58 - jährige n .[ dscore=0.5009310841560364, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Es</text> <text style=color:black>▁war</text> <text style=color:black>▁ein</text> <text style=color:black>▁erst</text> <text style=color:black>er</text> <text style=color:black>▁für</text> <text style=color:black>▁den</text> <text style=color:black>▁58</text> <text style=color:black>-</text> <text style=color:black>jährige</text> <text style=color:black>n</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.3118575811386108, dscore=0.5009310841560364, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁CH IO : ▁\" G old en e ▁Sport py ram ide ▁für ▁Bernhard ▁Lang er \"[ dscore=0.27801209688186646, bleu=76.25 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁CH</text> <text style=color:black>IO</text> <text style=color:black>:</text> <text style=color:black>▁\"</text> <text style=color:black>G</text> <text style=color:black>old</text> <text style=color:black>en</text> <text style=color:black>e</text> <text style=color:black>▁Sport</text> <text style=color:black>py</text> <text style=color:black>ram</text> <text style=color:black>ide</text> <text style=color:black>▁für</text> <text style=color:black>▁Bernhard</text> <text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:green>\".</text> <text style=color:black>[ cscore=1.1424641609191895, dscore=0.3542429804801941, bleu=77.49 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Nach ▁ein ▁paar ▁Minuten ▁entschied ▁das ▁Pferd , ▁es ▁wollte ▁nach ▁Hause ▁gehen , ▁und ▁ fuhr ▁zurück ▁zu ▁seine m ▁Futter tro g ▁bei ▁volle m ▁Til t .[ dscore=0.590915322303772, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Nach</text> <text style=color:black>▁ein</text> <text style=color:black>▁paar</text> <text style=color:black>▁Minuten</text> <text style=color:black>▁entschied</text> <text style=color:black>▁das</text> <text style=color:black>▁Pferd</text> <text style=color:black>,</text> <text style=color:black>▁es</text> <text style=color:black>▁wollte</text> <text style=color:black>▁nach</text> <text style=color:black>▁Hause</text> <text style=color:black>▁gehen</text> <text style=color:black>,</text> <text style=color:black>▁und</text> <text style=color:black>▁</text> <text style=color:black>fuhr</text> <text style=color:black>▁zurück</text> <text style=color:black>▁zu</text> <text style=color:black>▁seine</text> <text style=color:black>m</text> <text style=color:black>▁Futter</text> <text style=color:black>tro</text> <text style=color:black>g</text> <text style=color:black>▁bei</text> <text style=color:black>▁volle</text> <text style=color:black>m</text> <text style=color:black>▁Til</text> <text style=color:black>t</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.5400210618972778, dscore=0.590915322303772, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Nach ▁einem ▁Kilometer ▁mit ▁dieser ▁Geschwindigkeit ▁war ▁ich ▁Angst .[ dscore=0.4189392626285553, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Nach</text> <text style=color:black>▁einem</text> <text style=color:black>▁Kilometer</text> <text style=color:black>▁mit</text> <text style=color:black>▁dieser</text> <text style=color:black>▁Geschwindigkeit</text> <text style=color:green>▁hatte</text> <text style=color:black>▁ich</text> <text style=color:black>▁Angst</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.6339874267578125, dscore=0.34349122643470764, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Es ▁war ▁keine ▁gute ▁Erfahrung \", ▁sagte ▁Lang er .[ dscore=0.2565825283527374, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Es</text> <text style=color:black>▁war</text> <text style=color:black>▁keine</text> <text style=color:black>▁gute</text> <text style=color:black>▁Erfahrung</text> <text style=color:black>\",</text> <text style=color:black>▁sagte</text> <text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.1241008043289185, dscore=0.2565825283527374, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Und ▁damit ▁ist ▁seine ▁Karriere ▁im ▁Reit sport ▁wieder ▁zu ▁Ende ▁gegangen .[ dscore=0.7693734765052795, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Und</text> <text style=color:black>▁damit</text> <text style=color:black>▁ist</text> <text style=color:black>▁seine</text> <text style=color:black>▁Karriere</text> <text style=color:black>▁im</text> <text style=color:green>▁Reiter</text> <text style=color:black>sport</text> <text style=color:black>▁wieder</text> <text style=color:black>▁zu</text> <text style=color:black>▁Ende</text> <text style=color:black>▁gegangen</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.710274338722229, dscore=1.0200601816177368, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Am ▁Samstag ▁war ▁Deutschlands ▁best er ▁Golf er ▁aller ▁Zeiten ▁- ▁mit ▁101 ▁Turnier sieg en ▁unter ▁seine m ▁G ürt el ▁- ▁wieder ▁einmal ▁bei ▁Pferde n .[ dscore=0.3647630214691162, bleu=44.65 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Am</text> <text style=color:black>▁Samstag</text> <text style=color:black>▁war</text> <text style=color:black>▁Deutschlands</text> <text style=color:black>▁best</text> <text style=color:black>er</text> <text style=color:black>▁Golf</text> <text style=color:black>er</text> <text style=color:black>▁aller</text> <text style=color:black>▁Zeiten</text> <text style=color:black>▁-</text> <text style=color:black>▁mit</text> <text style=color:black>▁101</text> <text style=color:black>▁Turnier</text> <text style=color:black>sieg</text> <text style=color:black>en</text> <text style=color:black>▁unter</text> <text style=color:black>▁seine</text> <text style=color:black>m</text> <text style=color:black>▁G</text> <text style=color:black>ürt</text> <text style=color:black>el</text> <text style=color:black>▁-</text> <text style=color:black>▁wieder</text> <text style=color:black>▁einmal</text> <text style=color:black>▁bei</text> <text style=color:black>▁Pferde</text> <text style=color:black>n</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.5514886379241943, dscore=0.3647630214691162, bleu=44.65 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er ▁und ▁seine ▁Familie ▁re isten ▁auf ▁dem ▁A LR V - Park , ▁aber ▁er ▁ griff ▁nicht ▁auf ▁ein ▁Pferd ▁für ▁ein ▁Foto .[ dscore=0.6902999877929688, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er</text> <text style=color:black>▁und</text> <text style=color:black>▁seine</text> <text style=color:black>▁Familie</text> <text style=color:black>▁re</text> <text style=color:black>isten</text> <text style=color:black>▁auf</text> <text style=color:black>▁dem</text> <text style=color:black>▁A</text> <text style=color:black>LR</text> <text style=color:black>V</text> <text style=color:black>-</text> <text style=color:green>Gebiet</text> <text style=color:black>,</text> <text style=color:black>▁aber</text> <text style=color:black>▁er</text> <text style=color:black>▁</text> <text style=color:black>griff</text> <text style=color:black>▁nicht</text> <text style=color:black>▁auf</text> <text style=color:black>▁ein</text> <text style=color:black>▁Pferd</text> <text style=color:black>▁für</text> <text style=color:black>▁ein</text> <text style=color:black>▁Foto</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.8732446432113647, dscore=0.6891277432441711, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Es ▁war ▁das ▁erste ▁Mal ▁Lang er ▁in ▁Aachen ▁und ▁bei ▁CH IO .[ dscore=0.3892704248428345, bleu=31.31 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Es</text> <text style=color:black>▁war</text> <text style=color:black>▁das</text> <text style=color:black>▁erste</text> <text style=color:black>▁Mal</text> <text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:black>▁in</text> <text style=color:black>▁Aachen</text> <text style=color:black>▁und</text> <text style=color:black>▁bei</text> <text style=color:black>▁CH</text> <text style=color:black>IO</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.09409499168396, dscore=0.3892704248428345, bleu=31.31 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er ▁war ▁ gründlich ▁überzeugt .[ dscore=0.5035057067871094, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Er</text> <text style=color:black>▁war</text> <text style=color:black>▁</text> <text style=color:black>gründlich</text> <text style=color:black>▁überzeugt</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.7273181676864624, dscore=0.5035057067871094, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die ▁Deutsche ▁Sport ▁Aid ▁Stiftung ▁verlie h ▁dem ▁An haus er ▁bei ▁einer ▁kleinen ▁Gala ▁den ▁Golden en ▁Sport ▁Py ram ide .[ dscore=0.5638771653175354, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Die</text> <text style=color:black>▁Deutsche</text> <text style=color:black>▁Sport</text> <text style=color:black>▁Aid</text> <text style=color:black>▁Stiftung</text> <text style=color:black>▁verlie</text> <text style=color:black>h</text> <text style=color:black>▁dem</text> <text style=color:black>▁An</text> <text style=color:black>haus</text> <text style=color:black>er</text> <text style=color:black>▁bei</text> <text style=color:black>▁einer</text> <text style=color:black>▁kleinen</text> <text style=color:black>▁Gala</text> <text style=color:green>veranstaltung</text> <text style=color:black>▁Golden</text> <text style=color:black>en</text> <text style=color:black>▁Sport</text> <text style=color:black>▁Py</text> <text style=color:black>ram</text> <text style=color:black>ide</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.643588900566101, dscore=0.9079928994178772, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der ▁freundliche ▁Sportler ▁fehlt ▁nicht ▁für ▁Auszeichnung en .[ dscore=0.5723285675048828, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der</text> <text style=color:black>▁freundliche</text> <text style=color:black>▁Sportler</text> <text style=color:black>▁fehlt</text> <text style=color:black>▁nicht</text> <text style=color:black>▁für</text> <text style=color:black>▁Auszeichnung</text> <text style=color:black>en</text> <text style=color:black>.</text> <text style=color:black>[ cscore=2.150815963745117, dscore=0.5723285675048828, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Das ▁Bundes ver dienst kreuz , ▁das ▁Silber blatt , ▁Champion s ▁Tour ▁Player ▁of ▁the ▁Year ...[ dscore=0.44105836749076843, bleu=29.27 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Das</text> <text style=color:black>▁Bundes</text> <text style=color:black>ver</text> <text style=color:black>dienst</text> <text style=color:black>kreuz</text> <text style=color:black>,</text> <text style=color:green>▁der</text> <text style=color:black>▁Silber</text> <text style=color:black>blatt</text> <text style=color:black>,</text> <text style=color:black>▁Champion</text> <text style=color:black>s</text> <text style=color:black>▁Tour</text> <text style=color:black>▁Player</text> <text style=color:black>▁of</text> <text style=color:black>▁the</text> <text style=color:black>▁Year</text> <text style=color:black>...</text> <text style=color:black>[ cscore=1.511985421180725, dscore=0.5972800850868225, bleu=29.27 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Selbst ▁die ▁britische ▁König in ▁hat ▁ihm ▁eine ▁Ehre ▁zu teil ▁werden ▁lassen .[ dscore=0.3046220541000366, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Selbst</text> <text style=color:black>▁die</text> <text style=color:black>▁britische</text> <text style=color:black>▁König</text> <text style=color:black>in</text> <text style=color:black>▁hat</text> <text style=color:black>▁ihm</text> <text style=color:black>▁eine</text> <text style=color:black>▁Ehre</text> <text style=color:black>▁zu</text> <text style=color:black>teil</text> <text style=color:black>▁werden</text> <text style=color:black>▁lassen</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.113718867301941, dscore=0.3046220541000366, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Lang er ▁ist ▁die ▁18 te ▁Person , ▁die ▁mit ▁der ▁Sport ▁Py ram ide ▁ausgezeichnet ▁wurde .[ dscore=0.48935335874557495, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:black>▁ist</text> <text style=color:black>▁die</text> <text style=color:black>▁18</text> <text style=color:green>t</text> <text style=color:black>▁Person</text> <text style=color:black>,</text> <text style=color:black>▁die</text> <text style=color:black>▁mit</text> <text style=color:black>▁der</text> <text style=color:black>▁Sport</text> <text style=color:black>▁Py</text> <text style=color:black>ram</text> <text style=color:black>ide</text> <text style=color:black>▁ausgezeichnet</text> <text style=color:black>▁wurde</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.9731755256652832, dscore=0.8211442232131958, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Am ▁Samstag ▁traf ▁er ▁in ▁Aachen ▁den ▁ersten ▁Menschen , ▁der ▁ihn ▁empfangen ▁hat , ▁Hans ▁G ü n ter ▁W inkl er .[ dscore=0.45253661274909973, bleu=36.08 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Am</text> <text style=color:black>▁Samstag</text> <text style=color:green>▁begegne</text> <text style=color:black>▁er</text> <text style=color:black>▁in</text> <text style=color:black>▁Aachen</text> <text style=color:black>▁den</text> <text style=color:black>▁ersten</text> <text style=color:black>▁Menschen</text> <text style=color:black>,</text> <text style=color:black>▁der</text> <text style=color:black>▁ihn</text> <text style=color:black>▁empfangen</text> <text style=color:black>▁hat</text> <text style=color:black>,</text> <text style=color:black>▁Hans</text> <text style=color:black>▁G</text> <text style=color:black>ü</text> <text style=color:black>n</text> <text style=color:black>ter</text> <text style=color:black>▁W</text> <text style=color:black>inkl</text> <text style=color:black>er</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.51165771484375, dscore=1.161704659461975, bleu=34.39 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\" Diese ▁Auszeichnung ▁bedeutet ▁für ▁mich ▁viel \", ▁sagte ▁Lang er , ▁\" da ▁sie ▁nicht ▁nur ▁für ▁sportliche n ▁Erfolg , ▁sonder n ▁auch ▁für ▁soziale s ▁Engagement ▁vergeben ▁wird . \"[ dscore=0.3733322024345398, bleu=35.84 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁\"</text> <text style=color:black>Diese</text> <text style=color:black>▁Auszeichnung</text> <text style=color:black>▁bedeutet</text> <text style=color:black>▁für</text> <text style=color:black>▁mich</text> <text style=color:black>▁viel</text> <text style=color:black>\",</text> <text style=color:black>▁sagte</text> <text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:black>,</text> <text style=color:green>▁</text> <text style=color:green>weil</text> <text style=color:black>▁sie</text> <text style=color:black>▁nicht</text> <text style=color:black>▁nur</text> <text style=color:black>▁für</text> <text style=color:black>▁sportliche</text> <text style=color:black>n</text> <text style=color:black>▁Erfolg</text> <text style=color:black>,</text> <text style=color:black>▁sonder</text> <text style=color:black>n</text> <text style=color:black>▁auch</text> <text style=color:black>▁für</text> <text style=color:black>▁soziale</text> <text style=color:black>s</text> <text style=color:black>▁Engagement</text> <text style=color:black>▁vergeben</text> <text style=color:black>▁wird</text> <text style=color:black>.</text> <text style=color:black>\"</text> <text style=color:black>[ cscore=1.3903506994247437, dscore=0.8114174008369446, bleu=32.35 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Lang er ▁ fördert ▁seit ▁Jahren ▁auf streb ende ▁Talent e .[ dscore=0.36047592759132385, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Lang</text> <text style=color:black>er</text> <text style=color:black>▁</text> <text style=color:green>ermut</text> <text style=color:black>▁seit</text> <text style=color:black>▁Jahren</text> <text style=color:black>▁auf</text> <text style=color:black>streb</text> <text style=color:black>ende</text> <text style=color:black>▁Talent</text> <text style=color:black>e</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.1825683116912842, dscore=1.1799598932266235, bleu=0.0 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Und ▁in ▁diesem ▁Sinne ▁hat ▁er ▁sofort ▁das ▁Preis geld ▁von ▁2 5.000 ▁Euro ▁über wiesen .[ dscore=0.47843530774116516, bleu=39.92 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Und</text> <text style=color:black>▁in</text> <text style=color:black>▁diesem</text> <text style=color:black>▁Sinne</text> <text style=color:black>▁hat</text> <text style=color:black>▁er</text> <text style=color:black>▁sofort</text> <text style=color:black>▁das</text> <text style=color:black>▁Preis</text> <text style=color:black>geld</text> <text style=color:black>▁von</text> <text style=color:black>▁2</text> <text style=color:black>5.000</text> <text style=color:black>▁Euro</text> <text style=color:black>▁über</text> <text style=color:black>wiesen</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.1713429689407349, dscore=0.47843530774116516, bleu=39.92 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der ▁zwei malig e ▁US - Me ist er meister ▁wurde ▁am ▁Samstag ▁zusammen ▁mit ▁15 ▁weiteren ▁herausragend en ▁Sportler n ▁in ▁das ▁\" H all ▁of ▁Fa me ▁for ▁German ▁Sport \" ▁einge weih t .[ dscore=0.41291525959968567, bleu=30.36 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black>▁Der</text> <text style=color:black>▁zwei</text> <text style=color:black>malig</text> <text style=color:black>e</text> <text style=color:black>▁US</text> <text style=color:black>-</text> <text style=color:black>Me</text> <text style=color:black>ist</text> <text style=color:black>er</text> <text style=color:black>meister</text> <text style=color:black>▁wurde</text> <text style=color:black>▁am</text> <text style=color:black>▁Samstag</text> <text style=color:black>▁zusammen</text> <text style=color:black>▁mit</text> <text style=color:black>▁15</text> <text style=color:black>▁weiteren</text> <text style=color:black>▁herausragend</text> <text style=color:black>en</text> <text style=color:black>▁Sportler</text> <text style=color:black>n</text> <text style=color:black>▁in</text> <text style=color:green>▁die</text> <text style=color:black>▁\"</text> <text style=color:black>H</text> <text style=color:black>all</text> <text style=color:black>▁of</text> <text style=color:black>▁Fa</text> <text style=color:black>me</text> <text style=color:black>▁for</text> <text style=color:black>▁German</text> <text style=color:black>▁Sport</text> <text style=color:black>\"</text> <text style=color:black>▁einge</text> <text style=color:black>weih</text> <text style=color:black>t</text> <text style=color:black>.</text> <text style=color:black>[ cscore=1.2835179567337036, dscore=0.40762442350387573, bleu=36.28 ]</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_tests(k=100, initialization='greedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_tests(k=50, initialization='beam', beamsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
